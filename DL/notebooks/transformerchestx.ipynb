{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":951996,"sourceType":"datasetVersion","datasetId":516716},{"sourceId":301853,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":257787,"modelId":279061},{"sourceId":327988,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":275251,"modelId":279297},{"sourceId":337744,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":258055,"modelId":279297},{"sourceId":373354,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":308878,"modelId":329285},{"sourceId":384418,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":317220,"modelId":337735}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Deep Learning Project**\n\nMade by students:\n  - **Emanuele Conforti (252122)**\n  - **Jacopo Garofalo (252093)**\n  - **Gianmarco La Marca (252256)**","metadata":{}},{"cell_type":"markdown","source":"## **Environment initialization**\n\nThe **aim** of the project is to **generate a report starting from chest x-rays images**.","metadata":{}},{"cell_type":"markdown","source":"## **TransformerChestX Notebook Description**\n\n- In this notebook you can find all the code (and all the trials) we made on the transformer.\n\n- We used two pre-trained transformer (**GPT2** and **BioGPT**)\n\n- The notebook is divided into the following parts:\n\n- *short pre-processing*: just building, preparing the dataset and importing the models (encoder, ff_mapper and pre-trained transformer);\n\n\n- *text generation*: take an image, generate its latent space by using the ff_mapper and pass it to the pre-trained transformer to generate the report\n\n- *statistical analysis*: analyze and compare the ff_mapper's output with the transformer's expected embeddings to check whether the mapper is generating meaningful embeddings\n\n- *transformer fine-tuning*: fine-tuning on the pre-trained transformer to adapt it to our dataset (medical context)  ","metadata":{}},{"cell_type":"markdown","source":"### **Running the code on Colab**\n\n- Run the following cells with the variable **onColab = True** if you are on Colab.\n- We reccomend to run all the codes on Kaggle!","metadata":{}},{"cell_type":"code","source":"onColab = False\n\nif onColab:\n    ! pip install kaggle\n    ! mkdir ~/.kaggle\n    ! cp kaggle.json ~/.kaggle/\n    ! chmod 600 ~/.kaggle/kaggle.json","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T17:18:29.488985Z","iopub.execute_input":"2025-05-09T17:18:29.489240Z","iopub.status.idle":"2025-05-09T17:18:29.494486Z","shell.execute_reply.started":"2025-05-09T17:18:29.489211Z","shell.execute_reply":"2025-05-09T17:18:29.493741Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if onColab:\n    ! kaggle datasets download raddar/chest-xrays-indiana-university","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T17:18:29.495249Z","iopub.execute_input":"2025-05-09T17:18:29.495479Z","iopub.status.idle":"2025-05-09T17:18:29.509507Z","shell.execute_reply.started":"2025-05-09T17:18:29.495434Z","shell.execute_reply":"2025-05-09T17:18:29.508923Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import zipfile\nimport os\n\nif onColab:\n    file_name = \"chest-xrays-indiana-university.zip\"\n    \n    # extract the file from the zip\n    with zipfile.ZipFile(file_name, 'r') as zip_ref:\n        zip_ref.extractall(\"chest_xrays_data\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T17:18:29.510224Z","iopub.execute_input":"2025-05-09T17:18:29.510481Z","iopub.status.idle":"2025-05-09T17:18:29.521497Z","shell.execute_reply.started":"2025-05-09T17:18:29.510435Z","shell.execute_reply":"2025-05-09T17:18:29.520778Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if onColab:\n    !ls chest_xrays_data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T17:18:29.523035Z","iopub.execute_input":"2025-05-09T17:18:29.523216Z","iopub.status.idle":"2025-05-09T17:18:29.531504Z","shell.execute_reply.started":"2025-05-09T17:18:29.523200Z","shell.execute_reply":"2025-05-09T17:18:29.530888Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if onColab: \n    img_dir = 'chest_xrays_data/images/images_normalized/'\n    reports_dir = 'chest_xrays_data/indiana_reports.csv'\n    projections_dir = 'chest_xrays_data/indiana_projections.csv'\nelse:\n    img_dir = '/kaggle/input/chest-xrays-indiana-university/images/images_normalized/'\n    reports_dir = '/kaggle/input/chest-xrays-indiana-university/indiana_reports.csv'\n    projections_dir = '/kaggle/input/chest-xrays-indiana-university/indiana_projections.csv'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T17:18:29.532665Z","iopub.execute_input":"2025-05-09T17:18:29.532925Z","iopub.status.idle":"2025-05-09T17:18:29.541714Z","shell.execute_reply.started":"2025-05-09T17:18:29.532895Z","shell.execute_reply":"2025-05-09T17:18:29.541128Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"! pip install pytorch-msssim","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T17:18:29.542568Z","iopub.execute_input":"2025-05-09T17:18:29.542844Z","iopub.status.idle":"2025-05-09T17:18:34.595493Z","shell.execute_reply.started":"2025-05-09T17:18:29.542816Z","shell.execute_reply":"2025-05-09T17:18:34.594563Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# for BioGPT tokenizer\n!pip install sacremoses","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T17:18:34.596502Z","iopub.execute_input":"2025-05-09T17:18:34.596803Z","iopub.status.idle":"2025-05-09T17:18:38.296050Z","shell.execute_reply.started":"2025-05-09T17:18:34.596780Z","shell.execute_reply":"2025-05-09T17:18:38.294935Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-09T17:18:38.297132Z","iopub.execute_input":"2025-05-09T17:18:38.297382Z","iopub.status.idle":"2025-05-09T17:18:38.608753Z","shell.execute_reply.started":"2025-05-09T17:18:38.297361Z","shell.execute_reply":"2025-05-09T17:18:38.608057Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from PIL import Image\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T17:18:38.609606Z","iopub.execute_input":"2025-05-09T17:18:38.610072Z","iopub.status.idle":"2025-05-09T17:18:39.151882Z","shell.execute_reply.started":"2025-05-09T17:18:38.610029Z","shell.execute_reply":"2025-05-09T17:18:39.150936Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import GPT2Tokenizer, GPT2LMHeadModel, BioGptTokenizer, BioGptForCausalLM\n\nimport torch.optim as optim\nfrom torch.optim import AdamW\n\nfrom tqdm import tqdm\nfrom tqdm.auto import trange\n\nimport torchvision\nfrom torchvision import transforms as T\n\nfrom pytorch_msssim import ssim  # import SSIM as loss function","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T17:18:39.152719Z","iopub.execute_input":"2025-05-09T17:18:39.153059Z","iopub.status.idle":"2025-05-09T17:19:00.628712Z","shell.execute_reply.started":"2025-05-09T17:18:39.153039Z","shell.execute_reply":"2025-05-09T17:19:00.627828Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch.backends.cudnn.benchmark = True\n\ndevice = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n\nprint(f\"Using device: {torch.cuda.get_device_name(0)}\" if torch.cuda.is_available() else \"Using CPU\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T17:19:00.629642Z","iopub.execute_input":"2025-05-09T17:19:00.630108Z","iopub.status.idle":"2025-05-09T17:19:00.757909Z","shell.execute_reply.started":"2025-05-09T17:19:00.630086Z","shell.execute_reply":"2025-05-09T17:19:00.757255Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **Pre-processing**\n\nHere we print the datasets, analyze data inside them and prepare them for the training phase","metadata":{}},{"cell_type":"code","source":"reports_df = pd.read_csv(reports_dir)\nreports_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T17:19:00.758577Z","iopub.execute_input":"2025-05-09T17:19:00.758799Z","iopub.status.idle":"2025-05-09T17:19:00.839024Z","shell.execute_reply.started":"2025-05-09T17:19:00.758781Z","shell.execute_reply":"2025-05-09T17:19:00.838355Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"projections_df = pd.read_csv(projections_dir)\nprojections_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T17:19:00.841921Z","iopub.execute_input":"2025-05-09T17:19:00.842147Z","iopub.status.idle":"2025-05-09T17:19:00.868316Z","shell.execute_reply.started":"2025-05-09T17:19:00.842128Z","shell.execute_reply":"2025-05-09T17:19:00.867528Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### **Since we want to use the findings columns as labels (they represent the report we want to generate), we delete all the rows with null findings**","metadata":{}},{"cell_type":"code","source":"# filter the rows with null findings\nreports_filtered = reports_df.dropna(subset=[\"findings\"])\n\n# keep only entries in projections that have a filtered report associated (association through uid)\nprojections_filtered = projections_df[projections_df[\"uid\"].isin(reports_filtered[\"uid\"])]\nreports_filtered.shape, projections_filtered.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T17:19:00.870088Z","iopub.execute_input":"2025-05-09T17:19:00.870382Z","iopub.status.idle":"2025-05-09T17:19:00.891562Z","shell.execute_reply.started":"2025-05-09T17:19:00.870353Z","shell.execute_reply":"2025-05-09T17:19:00.890820Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"reports_filtered.isna().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T17:19:00.892342Z","iopub.execute_input":"2025-05-09T17:19:00.892616Z","iopub.status.idle":"2025-05-09T17:19:00.904622Z","shell.execute_reply.started":"2025-05-09T17:19:00.892596Z","shell.execute_reply":"2025-05-09T17:19:00.903758Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### **Split the filtered dataset (containing only the UID column) in train and validation set**","metadata":{}},{"cell_type":"code","source":"VAL_SIZE = 0.1\n\nuids = reports_filtered.uid.unique()\n\ntrain_ds, val_ds = train_test_split(\n    uids,\n    test_size=VAL_SIZE,\n)\n\nlen(train_ds), len(val_ds)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T17:19:00.905570Z","iopub.execute_input":"2025-05-09T17:19:00.905875Z","iopub.status.idle":"2025-05-09T17:19:00.913607Z","shell.execute_reply.started":"2025-05-09T17:19:00.905845Z","shell.execute_reply":"2025-05-09T17:19:00.912974Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### **Import the pre-trained transformer (GPT2 or BioGPT)**","metadata":{}},{"cell_type":"code","source":"def load_model_and_tokenizer(model_name=\"gpt2\"): \n    if model_name == \"BioGPT\":\n        tokenizer = BioGptTokenizer.from_pretrained(\"microsoft/biogpt\") \n        tokenizer.pad_token = tokenizer.eos_token\n        model = BioGptForCausalLM.from_pretrained(\"microsoft/biogpt\").to(device)\n        hidden_size = model.config.hidden_size\n        print(\"BioGPT model\")\n    else:\n        tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n        tokenizer.pad_token = tokenizer.eos_token\n        model = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(device)\n        hidden_size = model.config.n_embd\n        print(\"GPT2 model\")\n        \n    for param in model.parameters():\n        param.requires_grad = True  # Freezes all transformer parameters\n\n    transformer_parameters= sum(p.numel() for p in model.parameters())\n    print(f\"Number of transformer parameters: {transformer_parameters}\")\n\n    return tokenizer, model, hidden_size\n\n# if you want to import BioGPT, change this variable!\nmodel_name = \"GPT2\" # or \"BioGPT\"\ntokenizer, transformerModel, hidden_size = load_model_and_tokenizer(model_name)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T17:36:03.869311Z","iopub.execute_input":"2025-05-09T17:36:03.869662Z","iopub.status.idle":"2025-05-09T17:36:16.552298Z","shell.execute_reply.started":"2025-05-09T17:36:03.869631Z","shell.execute_reply":"2025-05-09T17:36:16.551185Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### **We create a custom dataset containing only the data we need:**\n- **images**\n- **tokenized findings**\n- **attention mask**","metadata":{}},{"cell_type":"code","source":"class ChestXRayDataset(Dataset):\n    def __init__(self, reports_df, projections_df, image_folder, tokenizer, uids, transforms):\n        self.reports_df = reports_df[reports_df[\"uid\"].isin(uids)].reset_index(drop=True)\n        self.projections_df = projections_df\n        self.image_folder = image_folder\n        self.tokenizer = tokenizer\n        self.transform = transforms\n\n    def __len__(self):\n        return len(self.reports_df)\n\n    def __getitem__(self, idx):\n        row = self.reports_df.iloc[idx]\n        uid = row[\"uid\"]\n        text = row[\"findings\"]\n\n        # tokenize findings column\n        encoded_text = self.tokenizer(\n            text,\n            padding=\"max_length\",\n            truncation=True,\n            max_length=144,\n            return_tensors=\"pt\"\n        )\n\n        # find the path and filename of the associated image\n        image_filename = self.projections_df[self.projections_df[\"uid\"] == uid][\"filename\"].values[0]\n        image_path = f\"{self.image_folder}/{image_filename}\"\n\n        # load and transform the image\n        image = Image.open(image_path).convert(\"L\")  # conversion to grayscale\n        image = self.transform(image)\n\n        # return the image, label (finding)\n        return image, encoded_text[\"input_ids\"].squeeze(0), encoded_text[\"attention_mask\"].squeeze(0)\n\n\ntf = T.Compose([\n    T.Resize((224, 224)),  # resizing for pre-trained models\n    T.ToTensor(),\n])\n\ntrain_dataset = ChestXRayDataset(reports_filtered, projections_filtered, img_dir, tokenizer, train_ds, tf)\nval_dataset = ChestXRayDataset(reports_filtered, projections_filtered, img_dir, tokenizer, val_ds, tf)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T17:36:21.594555Z","iopub.execute_input":"2025-05-09T17:36:21.595064Z","iopub.status.idle":"2025-05-09T17:36:21.617954Z","shell.execute_reply.started":"2025-05-09T17:36:21.595030Z","shell.execute_reply":"2025-05-09T17:36:21.616892Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### **Create the dataloader, that is we split the data in the dataset previously created in batches. We do this operation for both train set and validation set**","metadata":{}},{"cell_type":"code","source":"BATCH_SIZE = 64\n\n# create the DataLoader to generate batches of the dataset and iterate over them\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T17:36:26.986221Z","iopub.execute_input":"2025-05-09T17:36:26.986535Z","iopub.status.idle":"2025-05-09T17:36:27.002827Z","shell.execute_reply.started":"2025-05-09T17:36:26.986508Z","shell.execute_reply":"2025-05-09T17:36:27.001928Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### **OutOfMemoryError: the following code is used for freeing the GPU cache**","metadata":{}},{"cell_type":"code","source":"import gc\n\ngc.collect()\ntorch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T17:19:20.139246Z","iopub.execute_input":"2025-05-09T17:19:20.139463Z","iopub.status.idle":"2025-05-09T17:19:20.431627Z","shell.execute_reply.started":"2025-05-09T17:19:20.139430Z","shell.execute_reply":"2025-05-09T17:19:20.430698Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### **Loading and mapping the encoderCNN to its corresponding model**","metadata":{}},{"cell_type":"code","source":"def conv_layer(n_input, n_output, kernel_size, stride=1):\n    return nn.Sequential(\n        nn.Conv2d(n_input, n_output, kernel_size, stride),\n        nn.ReLU(),\n        nn.BatchNorm2d(n_output),\n        nn.MaxPool2d(2)\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T17:19:20.432552Z","iopub.execute_input":"2025-05-09T17:19:20.432831Z","iopub.status.idle":"2025-05-09T17:19:20.443909Z","shell.execute_reply.started":"2025-05-09T17:19:20.432801Z","shell.execute_reply":"2025-05-09T17:19:20.443050Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class EncoderCNN(nn.Module):\n    def __init__(self, in_channels):\n        super().__init__()\n        self.encoder = nn.Sequential(\n            conv_layer(in_channels, 64, 3),\n            conv_layer(64, 128, 3),\n            conv_layer(128, 256, 3),\n            conv_layer(256, 512, 3)\n        )\n\n    def encode(self, x):\n        return self.encoder(x)\n\n    def forward(self, x):\n        return self.encode(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T17:19:20.444767Z","iopub.execute_input":"2025-05-09T17:19:20.445026Z","iopub.status.idle":"2025-05-09T17:19:20.458234Z","shell.execute_reply.started":"2025-05-09T17:19:20.444993Z","shell.execute_reply":"2025-05-09T17:19:20.457564Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"state_dict = torch.load(\"/kaggle/input/encodercnn/pytorch/default/1/encoder.pth\")\n\nnew_state_dict = {f\"encoder.{k}\": v for k, v in state_dict.items()}\n\nencoderCNN = EncoderCNN(1).to(device)\nencoderCNN.load_state_dict(new_state_dict, strict=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T17:19:20.459135Z","iopub.execute_input":"2025-05-09T17:19:20.459408Z","iopub.status.idle":"2025-05-09T17:19:20.559190Z","shell.execute_reply.started":"2025-05-09T17:19:20.459381Z","shell.execute_reply":"2025-05-09T17:19:20.558286Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### **Build the FF mapper to adapt the encoderCNN output to the transformer**","metadata":{}},{"cell_type":"code","source":"def linear_layer(dim_input, dim_output, drop_p=0.2, last=False):\n    layers = [nn.Linear(dim_input, dim_output)]\n    if not last:\n        layers.append(nn.ReLU())\n        layers.append(nn.Dropout(p=drop_p))\n    return nn.Sequential(*layers)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T17:19:20.560185Z","iopub.execute_input":"2025-05-09T17:19:20.560506Z","iopub.status.idle":"2025-05-09T17:19:20.564603Z","shell.execute_reply.started":"2025-05-09T17:19:20.560473Z","shell.execute_reply":"2025-05-09T17:19:20.563836Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class FF_mapper(nn.Module):\n\n    \"\"\"\n    # old model (before the two approaches)\n    def __init__(self, dim_input, dim_output):\n        super().__init__()\n        self.ff = nn.Sequential(\n            linear_layer(dim_input, 640),\n            linear_layer(640, dim_output, last=True),\n            nn.LayerNorm(dim_output)\n        )\n    \"\"\"\n\n    \"\"\"\n    # token CE approach\n    def __init__(self, dim_input, dim_output):\n        super().__init__()\n        self.ff = nn.Sequential(\n            linear_layer(dim_input, 768),\n            linear_layer(768, 896),\n            #linear_layer(1024, 896),\n            linear_layer(896, dim_output, last=True),\n            nn.LayerNorm(dim_output)\n        )\n    \"\"\"\n    \n    # embedding approach \n    def __init__(self, dim_input, dim_output):\n        super().__init__()\n        self.ff = nn.Sequential(\n            linear_layer(dim_input, 640),\n            linear_layer(640, 896),\n            #linear_layer(896, 1024),\n            linear_layer(896, dim_output, last=True),\n            nn.LayerNorm(dim_output)\n        )\n\n    def forward(self, latent_space):\n        # flatten, permute and stuff\n        batch_size, C, H, W = latent_space.shape\n        latent_space = latent_space.permute(0, 2, 3, 1)  # (1, 12, 12, 512)\n        latent_space = latent_space.view(batch_size, H * W, C)  # (1, 144, 512)\n        return self.ff(latent_space)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T17:36:43.694410Z","iopub.execute_input":"2025-05-09T17:36:43.694754Z","iopub.status.idle":"2025-05-09T17:36:43.700253Z","shell.execute_reply.started":"2025-05-09T17:36:43.694729Z","shell.execute_reply":"2025-05-09T17:36:43.699518Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **Text generation**\n\n#### **Pass the encoder output as input to the transformer and generate the text**\n\n- Found a problem on the transformer generate function: since it's not differentiable (it internally uses non differentiable functions like argmax and sample), it does not allow us to perform backpropagation on the model (particularly on the ff_mapper). Hence, by using the generate function at training phase, we are not able to update the weights of the mapper ","metadata":{}},{"cell_type":"code","source":"# if we want to use generate function as a deterministic function, comment the top_k=50 \n# and add do_sample=False\ndef generate_text(model_name, model, tokenizer, inputs_embeds, attention_mask):\n    generated_ids = transformerModel.generate(\n        inputs_embeds=inputs_embeds, \n        max_length=512,\n        attention_mask=attention_mask,\n        pad_token_id=tokenizer.eos_token_id,\n        no_repeat_ngram_size=2,   # avoid repetitions\n        top_k=50,   # consider only the 50 most probable words\n        eos_token_id=None,\n    )\n    decoded_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n    print(\"Generated text:\", generated_ids)\n    print(\"Decoded generated text:\", decoded_text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T17:19:20.580092Z","iopub.execute_input":"2025-05-09T17:19:20.580375Z","iopub.status.idle":"2025-05-09T17:19:20.595565Z","shell.execute_reply.started":"2025-05-09T17:19:20.580355Z","shell.execute_reply":"2025-05-09T17:19:20.594817Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### **Transformer text generation**","metadata":{}},{"cell_type":"code","source":"# Loading fine-tuned GPT2\n\n# transformerModel.load_state_dict(torch.load(\"/kaggle/input/gpt2_fine_tuned/pytorch/default/1/gpt2_finetuned.pth\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T17:33:27.467300Z","iopub.execute_input":"2025-05-09T17:33:27.467647Z","iopub.status.idle":"2025-05-09T17:33:32.119096Z","shell.execute_reply.started":"2025-05-09T17:33:27.467618Z","shell.execute_reply":"2025-05-09T17:33:32.118309Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Parameters\nimage_feature_dim = 512  # EncoderCNN output (latent space) has 512 features for each spatial position\n                         # (the latent space size is 512)\nseq_length = 12 * 12  # the number of token (144, one for each spatial position)\n\nprint(\"Hidden size: \", hidden_size) # it represents the input size expected by the transformer (768 for GPT2 and 1024 for BioGPT) \n\n# Mapper definition\nmapper = FF_mapper(image_feature_dim, hidden_size).to(device)\n\n# Loading of the old model (before two approaches)\n# mapper.load_state_dict(torch.load(\"/kaggle/input/ff_mapper/pytorch/default/1/ff_mapper.pth\"))\n\n# Loading of the embedding approach\n# For the embedding approach, we can use GPT2 or BioGPT\n# GPT2:\nmapper.load_state_dict(torch.load(\"/kaggle/input/mapper/pytorch/cos_sim/2/ff_mapper_GPT2.pth\"))\n\n# BioGPT:\n# mapper.load_state_dict(torch.load(\"/kaggle/input/ff_mapper_biogpt/pytorch/default/1/ff_mapper_BioGPT.pth\"))\n\n# Loading of the token Cross Entropy approach\n# mapper.load_state_dict(torch.load(\"/kaggle/input/mapper/pytorch/token_cross_entropy/1/ff_mapper_GPT2.pth\"))\n\n# take a row from the validation dataset\nimages, labels, att_mask = val_dataset[90]\nimages = images.unsqueeze(0).to(device)\nlabels = labels.to(device)\natt_mask = att_mask.unsqueeze(0).to(device)\n\n# from encoderCNN to transformer\nwith torch.no_grad():\n    encoder_output = encoderCNN(images)  # features extraction\n    transformer_input = mapper(encoder_output)  # adapt to GPT-2\n\nprint(\"mapper output: \", transformer_input)\nprint(\"mapper output shape: \", transformer_input.shape)\nprint(\"mapper output dtype: \", transformer_input.dtype)\n\n# generate the report\ngenerate_text(model_name, transformerModel, tokenizer, transformer_input, att_mask)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T17:37:02.643383Z","iopub.execute_input":"2025-05-09T17:37:02.643714Z","iopub.status.idle":"2025-05-09T17:37:08.312096Z","shell.execute_reply.started":"2025-05-09T17:37:02.643687Z","shell.execute_reply":"2025-05-09T17:37:08.311104Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **Statistical analysis**\n\n- To understand and analyze what types of embeddings the ff_mapper is generating\n- To check if the ff_mapper is working properly","metadata":{}},{"cell_type":"code","source":"from sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport seaborn as sns\n\n# collect the transformer real embeddings and the predicted embeddings of some dataset samples\ndef collect_embeddings(unshuffled_dataloader, encoder, transformer, ff_mapper, device, max_samples=1000):\n    true_embeddings = []\n    pred_embeddings = []\n    findings_texts = []\n\n    # shuffle dataloader\n    dataloader = DataLoader(unshuffled_dataloader.dataset, batch_size=unshuffled_dataloader.batch_size, shuffle=True)\n    \n    with torch.no_grad():\n        for i, (images, text, attention) in enumerate(tqdm(dataloader, desc=\"Collecting embeddings\")):\n            # i: batch index\n            # images.shape[0]: batch size\n            if i * images.shape[0] >= max_samples:\n                break\n                \n            images = images.to(device)\n            text = text.to(device)\n            attention = attention.to(device)\n            \n            # obtain the image latent space from the encoder and predict the embeddings\n            latent_space = encoder(images)\n            \n            # in the old model, we did the following: \n            # pred_emb = ff_mapper(latent_space)\n            \n            # in the new model (in both approaches, embeddings one and token CE),\n            # the pred_emb have to be equal to the generated text:\n            ff_mapper_output = ff_mapper(latent_space)\n\n            pred_text = transformerModel.generate(\n                inputs_embeds=ff_mapper_output, \n                max_length=288,\n                attention_mask=attention,\n                pad_token_id=tokenizer.eos_token_id,\n                no_repeat_ngram_size=2,   # avoid repetitions\n                 # top_k=50,   # consider only the 50 most probable words\n                eos_token_id=None,\n                do_sample=False,\n            )\n\n            pred_emb = transformer.get_input_embeddings()(pred_text)\n            \n            # obtain the real embeddings from the transformer\n            true_emb = transformer.get_input_embeddings()(text)\n            \n            # save the embeddings\n            true_embeddings.append(true_emb.cpu().numpy())\n            pred_embeddings.append(pred_emb.cpu().numpy())\n            \n            # save also the associated tokenized reports\n            for t in text:\n                findings_texts.append(t.cpu().numpy())\n    \n    # model the embeddings tensors from 3D to 2D (from (batch_size, seq_length, embedding_size) to (batch_size * seq_length, embedding_size))\n    # where: \n    # - seq_length is the input sequence (like a statement) size\n    # - embedding_size is the embedding (numeric tensor of an object or a word) size\n    true_embeddings = np.vstack([emb.reshape(emb.shape[0] * emb.shape[1], emb.shape[2]) for emb in true_embeddings])\n    pred_embeddings = np.vstack([emb.reshape(emb.shape[0] * emb.shape[1], emb.shape[2]) for emb in pred_embeddings])\n\n    # these are matrices (arrays of embeddings, that are tensors)\n    return true_embeddings, pred_embeddings, findings_texts\n\nprint(\"Collecting embeddings...\")\ntrue_embeddings, pred_embeddings, findings_texts = collect_embeddings(\n    val_loader, encoderCNN, transformerModel, mapper, device\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T17:19:25.861085Z","iopub.execute_input":"2025-05-09T17:19:25.861405Z","iopub.status.idle":"2025-05-09T17:20:24.234545Z","shell.execute_reply.started":"2025-05-09T17:19:25.861372Z","shell.execute_reply":"2025-05-09T17:20:24.233673Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### **L2 Normalization**\n- The euclidean norm of an embedding geometrically represents the straight-line distance from the origin to that embedding\n\n- $||\\mathbf{v}||_2 = \\sqrt{\\sum_{i=1}^{n} v_i^2}$\n\n**L2 Unit Norm**: A vector has an L2 unit norm if its L2 norm is equal to 1\n\n- $||\\mathbf{v}||_2 = 1 \\iff \\sqrt{v_1^2 + v_2^2 + ... + v_n^2} = 1$\n\n- Each individual element of an L2 normalized embedding vector will always be between -1 and 1 (inclusive)\n\n- **In terms of embeddings**:\nWhen we normalize an embedding vector to have an L2 unit norm, we are essentially maintaining its direction in the embedding space, but scaling its length to 1\n\n\n- Why is normalization to the L2 unit norm useful for embeddings?\n\n    - **Focusing on direction**: In many embedding applications (especially in Natural Language Processing), the direction of an embedding vector captures the semantic meaning. By normalizing, we focus on the directional similarity between embeddings.\n    - **Comparability**: Normalizing embeddings makes their directions more comparable, regardless of their original magnitude. This can be useful when magnitudes can vary for reasons not directly related to semantic meaning","metadata":{}},{"cell_type":"code","source":"# calculate the euclidean norm (L2 norm) of the embeddings\ndef normalize_L2_embeddings(embeddings):\n    norm = np.linalg.norm(embeddings, axis=1, keepdims=True)\n    normalized_embeddings = embeddings / norm\n    return normalized_embeddings\n    \n# apply embedding normalization\ntrue_embeddings_normalized = normalize_L2_embeddings(true_embeddings)\npred_embeddings_normalized = normalize_L2_embeddings(pred_embeddings)\n\n# verify the norm of the normalized embeddings:\n# calculate the norms and check if they are equal to 1 \n# (that is, the square root of the sum of the squares of the elements is equal to 1)\n# print(\"embeddings: \", pred_embeddings[0])\n# print(\"normalized embeddings: \", pred_embeddings_normalized[0])\n# norms_after_normalization = np.linalg.norm(pred_embeddings_normalized, axis=1)\n# print(\"\\nL2 norms of normalized embeddings:\")\n# print(norms_after_normalization)\n\n# Check if the norms are close to 1\n# tolerance = 1e-7  # Small tolerance for floating-point comparisons\n# are_norms_close_to_one = np.allclose(norms_after_normalization, 1.0, atol=tolerance)\n# print(\"\\Are all norms close to 1?\", are_norms_close_to_one)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T17:20:24.235382Z","iopub.execute_input":"2025-05-09T17:20:24.235975Z","iopub.status.idle":"2025-05-09T17:20:24.426771Z","shell.execute_reply.started":"2025-05-09T17:20:24.235950Z","shell.execute_reply":"2025-05-09T17:20:24.426010Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# analyze the embeddings distribution and calculate some relevant statistics\ndef analyze_embeddings_distribution(true_embeddings, pred_embeddings):\n    # calculate mean and standard deviation for the embedding sets\n    true_mean = np.mean(true_embeddings, axis=0)\n    true_std = np.std(true_embeddings, axis=0)\n    \n    pred_mean = np.mean(pred_embeddings, axis=0)\n    pred_std = np.std(pred_embeddings, axis=0)\n\n    # true_mean, true_std, pred_mean, pred_std are all arrays\n    \n    # calculate the mean of the cosine similarity between the predicted embeddings and the real ones\n    # sim_matrix is the matrix of the cosine similarity where sim_matrix[i][j] is the\n    # cosine similarity between the i-th predicted embedding and the j-th real embedding\n    sim_matrix = cosine_similarity(pred_embeddings, true_embeddings)\n    # since we only want to analyze the similarity between a predicted embedding and its corresponding real one,\n    # we extract the main diagonal from the sim_matrix and calculate its mean\n    diag_sim = np.diag(sim_matrix)\n    avg_cosine_sim = np.mean(diag_sim)\n    \n    # calculate the mean of the euclidean distance    \n    # linalg.norm is a measure of the vector's distance from the origin (basically, the euclidean distance)\n    euclidean_distances = np.linalg.norm(true_embeddings - pred_embeddings, axis=1)\n    avg_euclidean_dist = np.mean(euclidean_distances)\n    \n    print(f\"Similarity Statistics:\")\n    print(f\"Average Cosine Similarity: {avg_cosine_sim:.4f}\")\n    print(f\"Average Euclidean Distance: {avg_euclidean_dist:.4f}\")\n    # the magnitude of embeddings can provide insights into the data distribution and the transformations \n    # applied by the mapper: if the magnitude of the predicted embeddings is significantly different \n    # from that of the real embeddings, it indicates that the mapper is altering the scale of the embeddings.\n    # For example, if the predicted embeddings have a much larger magnitude, \n    # it suggests that the mapper is \"inflating\" the vectors\n    print(f\"True Embeddings - Mean Magnitude: {np.linalg.norm(true_mean):.4f}, Std: {np.mean(true_std):.4f}\")\n    print(f\"Pred Embeddings - Mean Magnitude: {np.linalg.norm(pred_mean):.4f}, Std: {np.mean(pred_std):.4f}\")\n    \n    return {\n        'cosine_sim': diag_sim,\n        'euclidean_dist': euclidean_distances,\n        'true_mean': true_mean,\n        'true_std': true_std,\n        'pred_mean': pred_mean,\n        'pred_std': pred_std\n    }\n\nprint(\"\\nAnalyzing embeddings distribution...\")\nstats = analyze_embeddings_distribution(true_embeddings_normalized, pred_embeddings_normalized)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T17:20:24.427668Z","iopub.execute_input":"2025-05-09T17:20:24.427999Z","iopub.status.idle":"2025-05-09T17:20:39.895333Z","shell.execute_reply.started":"2025-05-09T17:20:24.427961Z","shell.execute_reply":"2025-05-09T17:20:39.894349Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# visualize the euclidean distances and cosine similarities distribution\ndef visualize_embeddings_comparison(true_embeddings, pred_embeddings, sample_size=1000, save_path=None):\n    # select a sample of the embeddings (to make the plot more readable)\n    if len(true_embeddings) > sample_size:\n        indices = np.random.choice(len(true_embeddings), sample_size, replace=False)\n        true_sample = true_embeddings[indices]\n        pred_sample = pred_embeddings[indices]\n    else:\n        true_sample = true_embeddings\n        pred_sample = pred_embeddings\n    \n    # calculate the Euclidean distances\n    distances = np.sqrt(np.sum((true_sample - pred_sample)**2, axis=1))\n    \n    # calculate the cosine similarities\n    cos_sims = np.sum(true_sample * pred_sample, axis=1) / (\n        np.sqrt(np.sum(true_sample**2, axis=1)) * np.sqrt(np.sum(pred_sample**2, axis=1))\n    )\n    # we can calculate the cosine similarities also using the cosine_similarity function\n    # sim_matrix = cosine_similarity(pred_embeddings, true_embeddings)\n    # diag_sim = np.diag(sim_matrix)\n    # avg_cosine_sim = np.mean(diag_sim)\n    \n    # plot\n    plt.figure(figsize=(14, 6))\n\n    # visualize the distribution of the euclidean distances\n    plt.subplot(1, 2, 1)\n    plt.hist(distances, bins=30, alpha=0.7)\n    plt.axvline(np.mean(distances), color='r', linestyle='--', label=f'Mean: {np.mean(distances):.4f}')\n    plt.title(\"Distribution of Euclidean Distances\")\n    plt.xlabel(\"Distance\")\n    plt.ylabel(\"Frequency\")\n    plt.legend()\n\n    # visualize the distribution of the cosine similarities\n    plt.subplot(1, 2, 2)\n    plt.hist(cos_sims, bins=30, alpha=0.7)\n    plt.axvline(np.mean(cos_sims), color='r', linestyle='--', label=f'Mean: {np.mean(cos_sims):.4f}')\n    plt.title(\"Distribution of Cosine Similarities\")\n    plt.xlabel(\"Similarity\")\n    plt.ylabel(\"Frequency\")\n    plt.legend()\n    \n    plt.tight_layout()\n\n    if save_path:\n        plt.savefig(f\"{save_path}_comparison.png\")\n    plt.show()\n    \n    return distances, cos_sims\n    \nsave_path = \"embeddings_analysis\"\n\nprint(\"\\nVisualizing embeddings comparison...\")\ndistances, cos_sims = visualize_embeddings_comparison(true_embeddings_normalized, pred_embeddings_normalized, save_path=save_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T17:20:39.896353Z","iopub.execute_input":"2025-05-09T17:20:39.896682Z","iopub.status.idle":"2025-05-09T17:20:40.723907Z","shell.execute_reply.started":"2025-05-09T17:20:39.896648Z","shell.execute_reply":"2025-05-09T17:20:40.723050Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### **PCA**\n\n- **PCA** (*Principal Component Analysis*): is a statistical technique used for dimensionality reduction. It transforms high-dimensional data into a lower-dimensional representation while preserving as much of the original variance as possible.\n    \n- How PCA Works:\n\n    - **Standardization**:\n        - PCA often begins by standardizing the data, which means scaling the variables to have a mean of 0 and a standard deviation of 1.\n    \n    - **Covariance Matrix**:\n        - PCA calculates the covariance matrix (which describes the relationships between the variables) of the standardized data \n   \n    - **Eigenvectors and Eigenvalues**:\n        - PCA then calculates the eigenvectors and eigenvalues of the covariance matrix.\n        - Eigenvectors represent the directions of the principal components and eigenvalues represent the magnitude of the variance along those directions\n    \n    - **Principal Components**:\n        - The eigenvectors are sorted by their corresponding eigenvalues in descending order (from the greatest eigenvalue to the smallest one). The top k eigenvectors (where k is the desired number of dimensions) are selected as the principal components\n    \n    - **Data Transformation**:\n        - The original data is projected onto the selected principal components, resulting in a lower-dimensional representation.\n\n\n\n- Hence, in our case we are reducing the embeddings from size of 768 (that is the GPT2 embedding size) to 2D.\n\n- **NOTE 1**: by reducing the embedding dimensionality, we are also losing information!\n- **NOTE 2**: PCA is a linear dimensionality reduction method! We will check also non-linear methods (like t-SNE)!\n    ","metadata":{}},{"cell_type":"code","source":"# apply PCA (Principal Components Analysis) and visualize the 2D embeddings\ndef visualize_embeddings_pca(true_embeddings, pred_embeddings, save_path=None):\n    # combine the true embeddings with the predicted ones in a single matrix (by stacking them)\n    combined_embeddings = np.vstack([true_embeddings, pred_embeddings])\n    \n    pca = PCA(n_components=2)\n    # reduce_embeddings will be a 2D matrix, where each row is an embedding projected in the principal components space\n    reduced_embeddings = pca.fit_transform(combined_embeddings)\n    \n    # split the results\n    true_reduced = reduced_embeddings[:len(true_embeddings)]\n    pred_reduced = reduced_embeddings[len(true_embeddings):]\n    \n    # plot\n    plt.figure(figsize=(10, 8))\n    plt.scatter(true_reduced[:, 0], true_reduced[:, 1], alpha=0.5, label=\"True Embeddings\", color=\"blue\")\n    plt.scatter(pred_reduced[:, 0], pred_reduced[:, 1], alpha=0.5, label=\"Predicted Embeddings\", color=\"red\")\n    # pca.explained_variance_ratio_: array where each element represents the proportion of the total variance \n    # in the original data that is explained by the corresponding principal component.\n    # it basically represents how much \"information\" or \"variance\" each direction (component, that are PC1 and PC2)\n    # captures from the original data.\n    plt.title(f\"PCA Visualization of Embeddings (explained var: {sum(pca.explained_variance_ratio_):.2f})\")\n    plt.xlabel(f\"PC1 ({pca.explained_variance_ratio_[0]:.2f})\")\n    plt.ylabel(f\"PC2 ({pca.explained_variance_ratio_[1]:.2f})\")\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n    \n    if save_path:\n        plt.savefig(f\"{save_path}_pca.png\", dpi=300, bbox_inches='tight')\n    plt.show()\n    \n    return pca\n\nprint(\"\\nVisualizing embeddings with PCA...\")\npca = visualize_embeddings_pca(true_embeddings_normalized, pred_embeddings_normalized, save_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T17:20:40.724695Z","iopub.execute_input":"2025-05-09T17:20:40.725021Z","iopub.status.idle":"2025-05-09T17:20:45.116814Z","shell.execute_reply.started":"2025-05-09T17:20:40.724969Z","shell.execute_reply":"2025-05-09T17:20:45.115820Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install umap-learn","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T17:20:45.117768Z","iopub.execute_input":"2025-05-09T17:20:45.118002Z","iopub.status.idle":"2025-05-09T17:20:48.933869Z","shell.execute_reply.started":"2025-05-09T17:20:45.117982Z","shell.execute_reply":"2025-05-09T17:20:48.932935Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### **UMAP**\n\n- **UMAP** (*Uniform Manifold Approximation and Projection*) is a powerful dimensionality reduction technique that is often used for visualizing high-dimensional data in a lower-dimensional space (typically 2D or 3D).\n\n1. **Constructing a High-Dimensional Graph**:\n    - UMAP first tries to understand the relationships between the data points in the original high-dimensional space.\nFor each data point, it identifies its nearest neighbors (the number of neighbors is controlled by the n_neighbors parameter).\nIt then creates a weighted graph where edges connect each point to its neighbors. The weight of an edge reflects how \"close\" the two points are in the high-dimensional space.\n\n\n2. **Approximating the Manifold**:\n\n    - UMAP assumes that the high-dimensional data lies on a lower-dimensional \"manifold\" (a complex, curved surface).\n    The graph constructed in the previous step is used to approximate the local structure of this manifold around each data point.\n\n\n3. **Defining a Low-Dimensional Graph**:\n\n    - UMAP then aims to create a similar graph in the low-dimensional space (2D) where the relationships between the points are as similar as possible to those in the high-dimensional graph.\n    It starts with a random initial layout of the points in the low-dimensional space.\n\n\n4. **Optimizing the Low-Dimensional Layout**:\n\n    - UMAP uses an optimization process (similar to gradient descent) to adjust the positions of the points in the low-dimensional space.\n    The goal of this optimization is to minimize the difference between the structure of the high-dimensional graph and the low-dimensional graph.\n    It tries to pull points that were close in the high-dimensional space closer in the low-dimensional space, and push points that were far apart further away.\n    The min_dist parameter plays a role here by setting a minimum allowed distance between points in the low-dimensional embedding.","metadata":{}},{"cell_type":"code","source":"import umap\n\n# applies UMAP\n# n_neighbors: number of nearest neighbors to consider for local manifold approximation.\n# Balances local versus global structure in the final embedding. Smaller values focus on local details,\n# larger values on global structure.\n# min_dist: minimum distance between embedded points. Controls how tightly points are packed together.\n# Smaller values allow tighter packing.\ndef visualize_embeddings_umap(true_embeddings, pred_embeddings, n_neighbors=15, min_dist=0.1, save_path=None):\n    # combine the true embeddings with the predicted ones in a single matrix (by stacking them)\n    combined_embeddings = np.vstack([true_embeddings, pred_embeddings])\n    # initialize and fit the UMAP reducer\n    # n_components=2: Reduces the dimensionality to 2 for visualization\n    reducer = umap.UMAP(n_components=2, n_neighbors=n_neighbors, min_dist=min_dist)\n    reduced_embeddings = reducer.fit_transform(combined_embeddings)\n\n    # split the reduced embeddings back into true and predicted sets\n    true_reduced = reduced_embeddings[:len(true_embeddings)]\n    pred_reduced = reduced_embeddings[len(true_embeddings):]\n\n    plt.figure(figsize=(10, 8))\n    plt.scatter(true_reduced[:, 0], true_reduced[:, 1], alpha=0.5, label=\"True Embeddings\", color=\"blue\")\n    plt.scatter(pred_reduced[:, 0], pred_reduced[:, 1], alpha=0.5, label=\"Predicted Embeddings\", color=\"red\")\n    plt.title(f\"UMAP Visualization of Embeddings (n_neighbors={n_neighbors}, min_dist={min_dist})\")\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n\n    if save_path:\n        plt.savefig(f\"{save_path}_umap_nn{n_neighbors}.png\", dpi=300, bbox_inches='tight')\n    plt.show()\n\n    return reducer\n\nprint(\"\\nVisualizing embeddings with UMAP...\")\nfor n_neighbors in [5, 15, 30]:\n    umap_reducer = visualize_embeddings_umap(true_embeddings_normalized, pred_embeddings_normalized, n_neighbors=n_neighbors, save_path=save_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T17:20:48.934935Z","iopub.execute_input":"2025-05-09T17:20:48.935260Z","iopub.status.idle":"2025-05-09T17:22:17.167794Z","shell.execute_reply.started":"2025-05-09T17:20:48.935236Z","shell.execute_reply":"2025-05-09T17:22:17.166417Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### **t-SNE**\n\n- **t-SNE**: it is a non-linear dimensionality reduction technique. It's primarily used for visualizing high-dimensional data in a low-dimensional space (typically 2D or 3D).\n\n- How it Works:\n    - **Probability Distributions**:\n \n        - t-SNE converts high-dimensional distances between data points into conditional probabilities.\n        - It creates a probability distribution in the high-dimensional space that represents the likelihood of points being neighbors.\n        - It then attempts to recreate a similar probability distribution in the low-dimensional space.\n\n    - **Minimizing Divergence**:\n \n        - t-SNE minimizes the Kullback-Leibler (KL) divergence between the probability distributions in the high-dimensional and low-dimensional spaces. This means it tries to make the distances between points in the low-dimensional space reflect their relationships in the high-dimensional space.\n\n    - **Non-linear Mapping**:\n        \n        - t-SNE uses a non-linear mapping, which allows it to capture complex relationships and structures in the data.\n\n\n\n- **Perplexity** controls the balance between local and global aspects of the data. It roughly corresponds to the number of nearest neighbors considered for each point. Lower perplexity values focus on local relationships, while higher values consider global relationships\n\n- t-SNE is generally considered better (than PCA) for visualization because it excels at revealing clusters and local patterns","metadata":{}},{"cell_type":"code","source":"# apply t-SNE (t-Distributed Stochastic Neighbor Embedding) and visualize the reduced embeddings\ndef visualize_embeddings_tsne(true_embeddings, pred_embeddings, perplexity=30, save_path=None):\n    # combine the true embeddings with the predicted ones in a single matrix (by stacking them)\n    combined_embeddings = np.vstack([true_embeddings, pred_embeddings])\n    \n    # To calculate t-SNE faster (since t-SNE can be computationally expensive for high-dimensional data),\n    # we can first apply PCA (on 50 components) and then t-SNE on the PCA results\n    if combined_embeddings.shape[1] > 50:\n        pca = PCA(n_components=50)\n        combined_embeddings = pca.fit_transform(combined_embeddings)\n    \n    tsne = TSNE(n_components=2, perplexity=perplexity, n_iter=1000) # verbose=1 to show progress\n    reduced_embeddings = tsne.fit_transform(combined_embeddings)\n    \n    # split the results\n    true_reduced = reduced_embeddings[:len(true_embeddings)]\n    pred_reduced = reduced_embeddings[len(true_embeddings):]\n    \n    # plot\n    plt.figure(figsize=(10, 8))\n    plt.scatter(true_reduced[:, 0], true_reduced[:, 1], alpha=0.5, label=\"True Embeddings\", color=\"blue\")\n    plt.scatter(pred_reduced[:, 0], pred_reduced[:, 1], alpha=0.5, label=\"Predicted Embeddings\", color=\"red\")\n    plt.title(f\"t-SNE Visualization of Embeddings (perplexity={perplexity})\")\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n    \n    if save_path:\n        plt.savefig(f\"{save_path}_tsne_perp{perplexity}.png\", dpi=300, bbox_inches='tight')\n    plt.show()\n    \n    return tsne\n\nprint(\"\\nVisualizing embeddings with t-SNE...\")\n# Trials made on different perplexity\nfor perplexity in [5, 30, 50]:\n    tsne = visualize_embeddings_tsne(true_embeddings_normalized, pred_embeddings_normalized, perplexity, save_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T17:22:17.168304Z","iopub.status.idle":"2025-05-09T17:22:17.168568Z","shell.execute_reply":"2025-05-09T17:22:17.168463Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **Transformer fine-tuning**\n\n- We tried to fine-tune the pre-trained transformer, in order to improve its text generation capacities and adapt it to our dataset (medical context). Unfortunately, we have fallen into **catastrophic forgetting** (pre-trained transformer regression, that is the transformer loses all its previous knowledge)\n- Hence, we avoided catastrophic forgetting by doing a light fine-tuning: we update only the last layers of the transformer\n- NOTE: we tried to fine-tune only GPT2, since it's the pre-trained transformer with the best obtained results","metadata":{}},{"cell_type":"code","source":"# Update only GPT-2 parameters of the last two blocks and lm_head\nfor name, param in transformerModel.named_parameters():\n    # transformer.h.<n> are the decoder blocks; lm_head is the output head\n    if name.startswith(\"transformer.h.10.\") or name.startswith(\"transformer.h.11.\") or name.startswith(\"lm_head.\"):\n        param.requires_grad = True\n    else:\n        param.requires_grad = False\n\n# Freeze encoderCNN and mapper completely\nfor p in encoderCNN.parameters():\n    p.requires_grad = False\n\nfor p in mapper.parameters():\n    p.requires_grad = False\n\ntrainable_params = [p for n, p in transformerModel.named_parameters() if p.requires_grad]\noptimizer = AdamW(trainable_params, lr=1e-4)\n\nloss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n\n# Dataloader used to fine-tune\ntrain_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=8)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T17:22:17.169127Z","iopub.status.idle":"2025-05-09T17:22:17.169507Z","shell.execute_reply":"2025-05-09T17:22:17.169312Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_one_epoch():\n    transformerModel.train()\n    total_loss = 0.0\n    for images, input_ids, attn_mask in tqdm(train_loader, desc=\"Train\"):\n        images, input_ids, attn_mask = images.to(device), input_ids.to(device), attn_mask.to(device)\n\n        # encode image -> mapper -> inputs_embeds\n        with torch.no_grad():\n            latent_space = encoderCNN(images)\n            inputs_embeds = mapper(latent_space)\n\n        # forward through GPT2 with labels\n        outputs = transformerModel(\n            inputs_embeds=inputs_embeds,\n            attention_mask=attn_mask,\n            labels=input_ids,\n            return_dict=True,\n        )\n        loss = outputs.loss\n\n        # Backpropagate only on trainable params\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item() * images.size(0)\n\n    return total_loss / len(train_loader.dataset)\n\n\ndef eval_one_epoch():\n    mapper.eval()\n    transformerModel.eval()\n    total_loss = 0.0\n    with torch.no_grad():\n        for images, input_ids, attn_mask in tqdm(val_loader, desc=\"Val\"):\n            images, input_ids, attn_mask = images.to(device), input_ids.to(device), attn_mask.to(device)\n            latent_space = encoderCNN(images)\n            inputs_embeds = mapper(latent_space)\n            outputs = transformerModel(\n                inputs_embeds=inputs_embeds,\n                attention_mask=attn_mask,\n                labels=input_ids,\n                return_dict=True,\n            )\n            total_loss += outputs.loss.item() * images.size(0)\n    return total_loss / len(val_loader.dataset)\n\n\nNUM_EPOCHS = 10\nfor epoch in range(1, NUM_EPOCHS+1):\n    train_loss = train_one_epoch()\n    val_loss   = eval_one_epoch()\n    print(f\"Epoch {epoch}  Train Loss: {train_loss:.4f}  Val Loss: {val_loss:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T17:22:17.170326Z","iopub.status.idle":"2025-05-09T17:22:17.170714Z","shell.execute_reply":"2025-05-09T17:22:17.170549Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### **Check fine-tuning results by generating a text from a dataset row**","metadata":{}},{"cell_type":"code","source":"images, labels, att_mask = val_dataset[90]\nimages = images.unsqueeze(0).to(device)\nlabels = labels.to(device)\natt_mask = att_mask.unsqueeze(0).to(device)\n\nwith torch.no_grad():\n    encoder_output = encoderCNN(images)  # features extraction\n    transformer_input = mapper(encoder_output)  # adapt to GPT-2\n\nprint(\"mapper output: \", transformer_input)\nprint(\"mapper output shape: \", transformer_input.shape)\nprint(\"mapper output dtype: \", transformer_input.dtype)\n\ngenerate_text(model_name, transformerModel, tokenizer, transformer_input, att_mask)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T17:22:17.171577Z","iopub.status.idle":"2025-05-09T17:22:17.171953Z","shell.execute_reply":"2025-05-09T17:22:17.171781Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch.save(transformerModel.state_dict(), \"gpt2_finetuned.pth\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T17:22:17.172723Z","iopub.status.idle":"2025-05-09T17:22:17.173108Z","shell.execute_reply":"2025-05-09T17:22:17.172931Z"}},"outputs":[],"execution_count":null}]}