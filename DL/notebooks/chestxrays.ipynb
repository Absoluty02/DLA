{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":951996,"sourceType":"datasetVersion","datasetId":516716},{"sourceId":301853,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":257787,"modelId":279061},{"sourceId":337744,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":258055,"modelId":279297},{"sourceId":373354,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":308878,"modelId":329285}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Deep Learning Project**\n\nMade by students:\n  - **Emanuele Conforti (252122)**\n  - **Jacopo Garofalo (252093)**\n  - **Gianmarco La Marca (252256)**","metadata":{}},{"cell_type":"markdown","source":"## **Environment initialization**\n\nThe **aim** of the project is to **generate a report starting from chest x-rays images**.","metadata":{}},{"cell_type":"markdown","source":"## **ChestXRays Notebook Description**\n\nThis is the notebook with the best solution we found, using the following trained models (imported from the other notebooks):\n\n- **encoderCNN**;\n- **mapper with embedding approach (cosine similarity as loss function)**;\n- **GPT2 transformer**.\n\nHere we also compute the final **metrics** (**ROUGE** and **BLEU**)","metadata":{}},{"cell_type":"markdown","source":"### **Running the code on Colab**\n\n- Run the following cells with the variable **onColab = True** if you are on Colab.\n- We recommend to run all the codes on Kaggle!","metadata":{}},{"cell_type":"code","source":"onColab = False\n\nif onColab:\n    ! pip install kaggle\n    ! mkdir ~/.kaggle\n    ! cp kaggle.json ~/.kaggle/\n    ! chmod 600 ~/.kaggle/kaggle.json","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T20:52:37.537580Z","iopub.execute_input":"2025-05-14T20:52:37.537897Z","iopub.status.idle":"2025-05-14T20:52:37.543881Z","shell.execute_reply.started":"2025-05-14T20:52:37.537864Z","shell.execute_reply":"2025-05-14T20:52:37.543155Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if onColab:\n    ! kaggle datasets download raddar/chest-xrays-indiana-university","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T20:52:37.544610Z","iopub.execute_input":"2025-05-14T20:52:37.544834Z","iopub.status.idle":"2025-05-14T20:52:37.557889Z","shell.execute_reply.started":"2025-05-14T20:52:37.544792Z","shell.execute_reply":"2025-05-14T20:52:37.557154Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import zipfile\nimport os\n\nif onColab:\n    file_name = \"chest-xrays-indiana-university.zip\"\n    \n    # extract the file from the zip\n    with zipfile.ZipFile(file_name, 'r') as zip_ref:\n        zip_ref.extractall(\"chest_xrays_data\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T20:52:37.558640Z","iopub.execute_input":"2025-05-14T20:52:37.558937Z","iopub.status.idle":"2025-05-14T20:52:37.570898Z","shell.execute_reply.started":"2025-05-14T20:52:37.558907Z","shell.execute_reply":"2025-05-14T20:52:37.570208Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if onColab:\n    !ls chest_xrays_data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T20:52:37.572239Z","iopub.execute_input":"2025-05-14T20:52:37.572470Z","iopub.status.idle":"2025-05-14T20:52:37.583942Z","shell.execute_reply.started":"2025-05-14T20:52:37.572451Z","shell.execute_reply":"2025-05-14T20:52:37.583171Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if onColab: \n    img_dir = 'chest_xrays_data/images/images_normalized/'\n    reports_dir = 'chest_xrays_data/indiana_reports.csv'\n    projections_dir = 'chest_xrays_data/indiana_projections.csv'\nelse:\n    img_dir = '/kaggle/input/chest-xrays-indiana-university/images/images_normalized/'\n    reports_dir = '/kaggle/input/chest-xrays-indiana-university/indiana_reports.csv'\n    projections_dir = '/kaggle/input/chest-xrays-indiana-university/indiana_projections.csv'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T20:52:37.585025Z","iopub.execute_input":"2025-05-14T20:52:37.585300Z","iopub.status.idle":"2025-05-14T20:52:37.596774Z","shell.execute_reply.started":"2025-05-14T20:52:37.585274Z","shell.execute_reply":"2025-05-14T20:52:37.596046Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-14T20:52:37.597501Z","iopub.execute_input":"2025-05-14T20:52:37.597711Z","iopub.status.idle":"2025-05-14T20:52:38.852420Z","shell.execute_reply.started":"2025-05-14T20:52:37.597681Z","shell.execute_reply":"2025-05-14T20:52:38.851495Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from PIL import Image\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T20:52:38.853438Z","iopub.execute_input":"2025-05-14T20:52:38.853926Z","iopub.status.idle":"2025-05-14T20:52:40.281558Z","shell.execute_reply.started":"2025-05-14T20:52:38.853893Z","shell.execute_reply":"2025-05-14T20:52:40.280746Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, Subset\n\nfrom transformers import GPT2Tokenizer, GPT2LMHeadModel, BioGptTokenizer, BioGptForCausalLM\nimport torch.optim as optim\nfrom torch.optim import AdamW\n\nfrom tqdm import tqdm\nfrom tqdm.auto import trange\n\nimport torchvision\nfrom torchvision import transforms as T","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T20:52:40.282470Z","iopub.execute_input":"2025-05-14T20:52:40.282982Z","iopub.status.idle":"2025-05-14T20:53:01.363851Z","shell.execute_reply.started":"2025-05-14T20:52:40.282950Z","shell.execute_reply":"2025-05-14T20:53:01.362937Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch.backends.cudnn.benchmark = True\n\ndevice = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n\nprint(f\"Using device: {torch.cuda.get_device_name(0)}\" if torch.cuda.is_available() else \"Using CPU\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T20:53:01.364713Z","iopub.execute_input":"2025-05-14T20:53:01.365292Z","iopub.status.idle":"2025-05-14T20:53:01.487023Z","shell.execute_reply.started":"2025-05-14T20:53:01.365267Z","shell.execute_reply":"2025-05-14T20:53:01.486307Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **Pre-processing**\n\nHere we print the datasets, analyze data inside them and prepare them for the training phase","metadata":{}},{"cell_type":"markdown","source":"#### **We first visualize the first rows of the datasets** ","metadata":{}},{"cell_type":"code","source":"reports_df = pd.read_csv(reports_dir)\nreports_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T20:53:01.487890Z","iopub.execute_input":"2025-05-14T20:53:01.488202Z","iopub.status.idle":"2025-05-14T20:53:01.568466Z","shell.execute_reply.started":"2025-05-14T20:53:01.488170Z","shell.execute_reply":"2025-05-14T20:53:01.567710Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"projections_df = pd.read_csv(projections_dir)\nprojections_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T20:53:01.570314Z","iopub.execute_input":"2025-05-14T20:53:01.570522Z","iopub.status.idle":"2025-05-14T20:53:01.594786Z","shell.execute_reply.started":"2025-05-14T20:53:01.570505Z","shell.execute_reply":"2025-05-14T20:53:01.594174Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"reports_df.shape, projections_df.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T20:53:01.596003Z","iopub.execute_input":"2025-05-14T20:53:01.596268Z","iopub.status.idle":"2025-05-14T20:53:01.601054Z","shell.execute_reply.started":"2025-05-14T20:53:01.596248Z","shell.execute_reply":"2025-05-14T20:53:01.600271Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def visualize_sample_data():\n  for uid in range(1, 4):\n    plt.figure(figsize=(10, 5))\n    print(\"\\nUID: \", uid)\n\n    findings = list(reports_df[reports_df['uid'] == uid]['findings'])[0]\n    images = projections_df[projections_df['uid'] == uid]['filename']\n\n    for img in images:\n      png_img = Image.open(os.path.join(img_dir, img))\n      png_img = png_img.convert('RGB')\n      plt.title(img)\n      plt.imshow(png_img)\n      plt.axis('off')\n      plt.show()\n    print(\"Findings:\", findings)\n\nvisualize_sample_data()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T20:53:01.601977Z","iopub.execute_input":"2025-05-14T20:53:01.602276Z","iopub.status.idle":"2025-05-14T20:53:05.482083Z","shell.execute_reply.started":"2025-05-14T20:53:01.602248Z","shell.execute_reply":"2025-05-14T20:53:05.480945Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### **We check the number of null values for each column**","metadata":{}},{"cell_type":"code","source":"reports_df.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T20:53:05.482975Z","iopub.execute_input":"2025-05-14T20:53:05.483216Z","iopub.status.idle":"2025-05-14T20:53:05.505534Z","shell.execute_reply.started":"2025-05-14T20:53:05.483195Z","shell.execute_reply":"2025-05-14T20:53:05.504679Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"reports_df.isna().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T20:53:05.506274Z","iopub.execute_input":"2025-05-14T20:53:05.506508Z","iopub.status.idle":"2025-05-14T20:53:05.527187Z","shell.execute_reply.started":"2025-05-14T20:53:05.506477Z","shell.execute_reply":"2025-05-14T20:53:05.526418Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"projections_df.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T20:53:05.528001Z","iopub.execute_input":"2025-05-14T20:53:05.528271Z","iopub.status.idle":"2025-05-14T20:53:05.545526Z","shell.execute_reply.started":"2025-05-14T20:53:05.528239Z","shell.execute_reply":"2025-05-14T20:53:05.544691Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### **We analyze the images distribution on the dataset rows (number of images per row)**","metadata":{}},{"cell_type":"code","source":"image_counts = projections_df.groupby(\"uid\")[\"filename\"].count()\n\nnum_uids = []\nnum_uids.append(reports_df[\"uid\"].nunique() - image_counts.count())\n\nfor i in range(1, 7):\n    num_uids.append((image_counts == i).sum())\n\nprint(f\"Sum of all counted entries: {sum(num_uids)}\")\nprint(f\"Total entries: {image_counts.count()}\")\n\nlabels = [f\"{i} images\" for i in range(0, 7)]\n\nplt.figure(figsize=(8, 5))\nplt.bar(labels, num_uids, color='skyblue')\n\nplt.xlabel(\"Number of associated images\")\nplt.ylabel(\"Number of UIDs\")\nplt.title(\"Distribution of images number per UID\")\n\nfor i, v in enumerate(num_uids):\n    plt.text(i, v + 2, str(v), ha='center', fontsize=12)\n\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T20:53:05.546425Z","iopub.execute_input":"2025-05-14T20:53:05.546720Z","iopub.status.idle":"2025-05-14T20:53:05.745717Z","shell.execute_reply.started":"2025-05-14T20:53:05.546691Z","shell.execute_reply":"2025-05-14T20:53:05.744983Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# visualize the 5 images related to the same entry\ndef visualize_data(uid):\n    print(f\"UID with 5 images associated: {uid}\")\n    plt.figure(figsize=(10, 5))\n    \n    images = projections_df[projections_df['uid'] == uid]['filename']\n    \n    for img in images:\n      png_img = Image.open(os.path.join(img_dir, img))\n      png_img = png_img.convert('RGB')\n      plt.title(img)\n      plt.imshow(png_img)\n      plt.axis('off')\n      plt.show()\n        \nuid = list(image_counts[image_counts == 5].index)[0]\nvisualize_data(uid)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T20:53:05.746535Z","iopub.execute_input":"2025-05-14T20:53:05.746789Z","iopub.status.idle":"2025-05-14T20:53:09.335643Z","shell.execute_reply.started":"2025-05-14T20:53:05.746762Z","shell.execute_reply":"2025-05-14T20:53:09.334441Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### **Since we want to use the findings columns as labels (they represent the report we want to generate), we delete all the rows with null findings**","metadata":{}},{"cell_type":"code","source":"# filter the rows with null findings\nreports_filtered = reports_df.dropna(subset=[\"findings\"])\n\n# keep only entries in projections that have a filtered report associated (association through uid)\nprojections_filtered = projections_df[projections_df[\"uid\"].isin(reports_filtered[\"uid\"])]\nreports_filtered.shape, projections_filtered.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T20:53:09.336918Z","iopub.execute_input":"2025-05-14T20:53:09.337333Z","iopub.status.idle":"2025-05-14T20:53:09.354030Z","shell.execute_reply.started":"2025-05-14T20:53:09.337290Z","shell.execute_reply":"2025-05-14T20:53:09.352975Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"reports_filtered.isna().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T20:53:09.354763Z","iopub.execute_input":"2025-05-14T20:53:09.355036Z","iopub.status.idle":"2025-05-14T20:53:09.388772Z","shell.execute_reply.started":"2025-05-14T20:53:09.355014Z","shell.execute_reply":"2025-05-14T20:53:09.387878Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### **Split the filtered dataset (containing only the UID column) in train and validation set**","metadata":{}},{"cell_type":"code","source":"VAL_SIZE = 0.1\n\nuids = reports_filtered.uid.unique()\n\ntrain_ds, val_ds = train_test_split(\n    uids,\n    test_size=VAL_SIZE,\n    random_state=42\n)\n\nlen(train_ds), len(val_ds)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T20:53:09.389527Z","iopub.execute_input":"2025-05-14T20:53:09.389775Z","iopub.status.idle":"2025-05-14T20:53:09.405309Z","shell.execute_reply.started":"2025-05-14T20:53:09.389754Z","shell.execute_reply":"2025-05-14T20:53:09.404681Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### **We create a custom dataset containing only the data we need:**\n- **images**\n- **tokenized findings**\n- **attention mask**","metadata":{}},{"cell_type":"code","source":"# adjusted dataset\nclass ChestXRayDataset(Dataset):\n    def __init__(self, reports_df, projections_df, image_folder, tokenizr, uids, transforms):\n        self.reports_df = reports_df[reports_df[\"uid\"].isin(uids)].reset_index(drop=True)\n        self.projections_df = projections_df\n        self.image_folder = image_folder\n        self.tokenizer = tokenizr\n        # a series of transformations to be applied to images before feeding them into a model\n        self.transform = transforms\n\n    def __len__(self):\n        return len(self.reports_df)\n\n    def __getitem__(self, idx):\n        row = self.reports_df.iloc[idx]\n        uid = row[\"uid\"]\n        text = row[\"findings\"]\n\n        # tokenize findings column\n        encoded_text = self.tokenizer(\n            text, padding=\"max_length\", truncation=True, max_length=144, return_tensors=\"pt\"\n        )\n\n        # find the path and filename of the associated image\n        image_filename = self.projections_df[self.projections_df[\"uid\"] == uid][\"filename\"].values[0]\n        image_path = f\"{self.image_folder}/{image_filename}\"\n\n        # load and trasform the image\n        image = Image.open(image_path).convert(\"L\")  # conversion to grayscale\n        image = self.transform(image)\n\n        # return the image, label (finding)\n        return image, encoded_text[\"input_ids\"].squeeze(0), encoded_text[\"attention_mask\"].squeeze(0)\n\n# initialize the GPT-2 tokenizer\ntokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\ntokenizer.pad_token = tokenizer.eos_token\n\ntf = T.Compose([\n    T.Resize((224, 224)),  # resizing for pre-trained models\n    T.ToTensor(),\n])\n\ntrain_dataset = ChestXRayDataset(reports_filtered, projections_filtered, img_dir, tokenizer, train_ds, tf)\nval_dataset = ChestXRayDataset(reports_filtered, projections_filtered, img_dir, tokenizer, val_ds, tf)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T20:53:09.406150Z","iopub.execute_input":"2025-05-14T20:53:09.406351Z","iopub.status.idle":"2025-05-14T20:53:11.497823Z","shell.execute_reply.started":"2025-05-14T20:53:09.406332Z","shell.execute_reply":"2025-05-14T20:53:11.496871Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### **Visualize the data of the new dataset**","metadata":{}},{"cell_type":"code","source":"# the image should be a pytorch tensor \nimage, label, att_mask = train_dataset[100]\nimage","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T20:53:11.501705Z","iopub.execute_input":"2025-05-14T20:53:11.501959Z","iopub.status.idle":"2025-05-14T20:53:11.747111Z","shell.execute_reply.started":"2025-05-14T20:53:11.501939Z","shell.execute_reply":"2025-05-14T20:53:11.746309Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"image.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T20:53:11.748767Z","iopub.execute_input":"2025-05-14T20:53:11.749014Z","iopub.status.idle":"2025-05-14T20:53:11.754120Z","shell.execute_reply.started":"2025-05-14T20:53:11.748994Z","shell.execute_reply":"2025-05-14T20:53:11.753222Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"label","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T20:53:11.755043Z","iopub.execute_input":"2025-05-14T20:53:11.755363Z","iopub.status.idle":"2025-05-14T20:53:11.768569Z","shell.execute_reply.started":"2025-05-14T20:53:11.755341Z","shell.execute_reply":"2025-05-14T20:53:11.767850Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"att_mask","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T20:53:11.769340Z","iopub.execute_input":"2025-05-14T20:53:11.769533Z","iopub.status.idle":"2025-05-14T20:53:11.782887Z","shell.execute_reply.started":"2025-05-14T20:53:11.769516Z","shell.execute_reply":"2025-05-14T20:53:11.782128Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### **Create the dataloader, that is we split the data of the dataset previously created in batches. We do this operation for both train set and validation set**","metadata":{}},{"cell_type":"code","source":"BATCH_SIZE = 32\n\n# create the DataLoader to generate batches of the dataset and iterate over them\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T20:53:11.783666Z","iopub.execute_input":"2025-05-14T20:53:11.783925Z","iopub.status.idle":"2025-05-14T20:53:11.793558Z","shell.execute_reply.started":"2025-05-14T20:53:11.783895Z","shell.execute_reply":"2025-05-14T20:53:11.792860Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### **OutOfMemoryError: the following code is used for freeing the GPU cache**","metadata":{}},{"cell_type":"code","source":"import gc\n\ngc.collect()\ntorch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T20:53:11.794156Z","iopub.execute_input":"2025-05-14T20:53:11.794385Z","iopub.status.idle":"2025-05-14T20:53:12.103536Z","shell.execute_reply.started":"2025-05-14T20:53:11.794365Z","shell.execute_reply":"2025-05-14T20:53:12.102780Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### **Now we can start building our model. It will mainly be a CustomAutoencoder composed by the following elements:**\n- **encoder**: a **convolutional encoder**, that will take the images and encode them in a **latent space**;\n- **decoder**: a **transformer**, that will take the latent space generated by the encoder and the findings columns and generate the text (report);","metadata":{}},{"cell_type":"code","source":"def conv_layer(n_input, n_output, kernel_size, stride=1):\n    return nn.Sequential(\n        nn.Conv2d(n_input, n_output, kernel_size, stride),\n        nn.ReLU(),\n        nn.BatchNorm2d(n_output),\n        nn.MaxPool2d(2)\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T20:53:12.104221Z","iopub.execute_input":"2025-05-14T20:53:12.104426Z","iopub.status.idle":"2025-05-14T20:53:12.120012Z","shell.execute_reply.started":"2025-05-14T20:53:12.104409Z","shell.execute_reply":"2025-05-14T20:53:12.119309Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"encoder = nn.Sequential(\n            conv_layer(1, 64, 3),\n            conv_layer(64, 128, 3),\n            conv_layer(128, 256, 3),\n            conv_layer(256, 512, 3)\n        )\n\n# In this case, we use encoderCNN as encoder. However, we also implemented a VAE and tried to use it \n# as encoder but it's not effective as the encoderCNN. You can find more on these two models on the\n# EncoderChestX Notebook!\nencoder.load_state_dict(torch.load(\"/kaggle/input/encodercnn/pytorch/default/1/encoder.pth\"))\nencoder.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T20:53:12.120843Z","iopub.execute_input":"2025-05-14T20:53:12.121111Z","iopub.status.idle":"2025-05-14T20:53:12.458402Z","shell.execute_reply.started":"2025-05-14T20:53:12.121090Z","shell.execute_reply":"2025-05-14T20:53:12.457324Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def linear_layer(dim_input, dim_output, drop_p=0.1, last=False):\n    layers = [nn.Linear(dim_input, dim_output)]\n    if not last:\n        layers.append(nn.ReLU())\n        layers.append(nn.Dropout(p=drop_p))\n    return nn.Sequential(*layers)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T20:53:12.459332Z","iopub.execute_input":"2025-05-14T20:53:12.459565Z","iopub.status.idle":"2025-05-14T20:53:12.463763Z","shell.execute_reply.started":"2025-05-14T20:53:12.459544Z","shell.execute_reply":"2025-05-14T20:53:12.463081Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class FF_mapper(nn.Module):\n\n    def __init__(self, dim_input, dim_output):\n        super().__init__()\n        self.ff = nn.Sequential(\n            linear_layer(dim_input, 640),\n            linear_layer(640, 896),\n            #linear_layer(896, 1024),\n            linear_layer(896, dim_output, last=True),\n            nn.LayerNorm(dim_output)\n        )\n\n    def forward(self, latent_space):\n        # flatter, permute and stuff\n        batch_size, C, H, W = latent_space.shape\n        latent_space = latent_space.permute(0, 2, 3, 1)  # (1, 12, 12, 512)\n        latent_space = latent_space.view(batch_size, H * W, C)  # (1, 144, 512)\n        return self.ff(latent_space)\n\nmapper = FF_mapper(512, 768).to(device)\n\n# In this case, we use the ff_mapper with embedding approach (cosine similarity as loss function) \n# as mapper. However, we also implemented an ff_mapper with a token approach (which uses cross entropy\n# as loss function) but it's not effective as this one. You can find more on these two models on the\n# ff-mapper Notebook! \nmapper.load_state_dict(torch.load(\"/kaggle/input/mapper/pytorch/cos_sim/2/ff_mapper_GPT2.pth\"))\nmapper.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T20:53:12.464473Z","iopub.execute_input":"2025-05-14T20:53:12.464750Z","iopub.status.idle":"2025-05-14T20:53:12.620138Z","shell.execute_reply.started":"2025-05-14T20:53:12.464729Z","shell.execute_reply":"2025-05-14T20:53:12.619299Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### **Now we import the pre-trained transformer (GPT2) and start working on it**","metadata":{}},{"cell_type":"code","source":"# In this case, we use GPT2 as transformer. However, we also tried to use BioGPT\n# but it's not effective as GPT2. You can find more on these two models on the\n# TransformerChestX Notebook!\ntransformer = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n\nfor param in transformer.parameters():\n    param.requires_grad = False  # Freezes all transformer parameters\n\ntransformer.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T20:53:12.620874Z","iopub.execute_input":"2025-05-14T20:53:12.621100Z","iopub.status.idle":"2025-05-14T20:53:16.562557Z","shell.execute_reply.started":"2025-05-14T20:53:12.621078Z","shell.execute_reply":"2025-05-14T20:53:16.561624Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# function used to generate a report:\n# the transformer takes as input some embeddings (inputs_embeds) and corresponding attention masks\ndef generate_text(transformer, inputs_embeds, attention_mask):\n    return transformer.generate(\n        inputs_embeds=inputs_embeds, \n        max_length=288,\n        attention_mask=attention_mask,\n        pad_token_id=tokenizer.eos_token_id,\n        no_repeat_ngram_size=2,   # avoid repetitions\n        #top_k=50,   # consider only the 50 most probable words\n        eos_token_id=None,\n        do_sample=False\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T20:53:16.563458Z","iopub.execute_input":"2025-05-14T20:53:16.563720Z","iopub.status.idle":"2025-05-14T20:53:16.567566Z","shell.execute_reply.started":"2025-05-14T20:53:16.563687Z","shell.execute_reply":"2025-05-14T20:53:16.566781Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### **Here there's the structure of the main CustomAutoencoder**","metadata":{}},{"cell_type":"code","source":"# General Autoencoder\nclass CustomAutoencoder(torch.nn.Module):\n    def __init__(self, encoder, mapper, transformer):\n        super().__init__()\n\n        # The encoder takes images as input and encodes them in a latent space\n        self.encoder = encoder\n\n        # Adapt the latent space dimensions\n        self.mapper = mapper\n    \n        # The decoder should take the latent space (images) and generate the report\n        self.decoder = transformer\n    \n    def forward(self, images, attention):\n\n        # the latent space computed by the encoder\n        latent_space = encoder(images).to(device)\n\n        # the embeddings (computed by the mapper), which the transformer will take as input\n        pred_embeds = self.mapper(latent_space)\n\n        # return the (tokenized) text generated by the transformer\n        return generate_text(self.decoder, pred_embeds, attention)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T20:53:16.568302Z","iopub.execute_input":"2025-05-14T20:53:16.568522Z","iopub.status.idle":"2025-05-14T20:53:16.606343Z","shell.execute_reply.started":"2025-05-14T20:53:16.568491Z","shell.execute_reply":"2025-05-14T20:53:16.605660Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"final_model = CustomAutoencoder(encoder, mapper, transformer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T20:53:16.607026Z","iopub.execute_input":"2025-05-14T20:53:16.607274Z","iopub.status.idle":"2025-05-14T20:53:16.620504Z","shell.execute_reply.started":"2025-05-14T20:53:16.607255Z","shell.execute_reply":"2025-05-14T20:53:16.619848Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def generate_text_from_dataset(loader, modell, tokenizr):\n    data_iter = iter(loader)\n    image, text, attention = next(data_iter)\n    \n    print(f\"Real Text:\\n{tokenizer.decode(text[0], skip_special_tokens=True)}\\n\\n\")\n    \n    image = image.to(device)\n    text = text.to(device)\n    attention = attention.to(device)\n    \n    with torch.no_grad():\n        predicted_text = modell(image, attention).to(device)\n    \n    print(f\"Predicted Text:\\n{tokenizr.decode(predicted_text[0], skip_special_tokens=True)}\")\n\ngenerate_text_from_dataset(train_loader, final_model, tokenizer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T20:53:16.621198Z","iopub.execute_input":"2025-05-14T20:53:16.621424Z","iopub.status.idle":"2025-05-14T20:53:24.979439Z","shell.execute_reply.started":"2025-05-14T20:53:16.621406Z","shell.execute_reply":"2025-05-14T20:53:24.978493Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **Building a test set**\n\n- We built a test set taking data from a new dataset (https://huggingface.co/datasets/Sina-Alinejad-2002/train_chexpert)","metadata":{}},{"cell_type":"code","source":"!pip install datasets","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T20:53:24.980353Z","iopub.execute_input":"2025-05-14T20:53:24.980671Z","iopub.status.idle":"2025-05-14T20:53:29.443033Z","shell.execute_reply.started":"2025-05-14T20:53:24.980637Z","shell.execute_reply":"2025-05-14T20:53:29.441914Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from datasets import load_dataset\n\ndataset = load_dataset(\"Sina-Alinejad-2002/train_chexpert\", split=\"train[:2000]\")\n\n# convert into DataFrame\ndf = dataset.to_pandas()\n\ndf.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T20:53:29.444206Z","iopub.execute_input":"2025-05-14T20:53:29.444557Z","iopub.status.idle":"2025-05-14T20:53:33.374872Z","shell.execute_reply.started":"2025-05-14T20:53:29.444518Z","shell.execute_reply":"2025-05-14T20:53:33.374156Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T20:53:33.375646Z","iopub.execute_input":"2025-05-14T20:53:33.376314Z","iopub.status.idle":"2025-05-14T20:53:33.382355Z","shell.execute_reply.started":"2025-05-14T20:53:33.376286Z","shell.execute_reply":"2025-05-14T20:53:33.381249Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from IPython.display import display\n\ndisplay(dataset[0]['image'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T20:53:33.383488Z","iopub.execute_input":"2025-05-14T20:53:33.383888Z","iopub.status.idle":"2025-05-14T20:53:33.463980Z","shell.execute_reply.started":"2025-05-14T20:53:33.383847Z","shell.execute_reply":"2025-05-14T20:53:33.463196Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from PIL import Image\nfrom io import BytesIO\n\nclass HuggingFaceChestXRayDataset(Dataset):\n    def __init__(self, df, tokenizer, transform=None):\n        self.df = df.reset_index(drop=True)\n        self.tokenizer = tokenizer\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        \n        # Load image from bytes\n        image_bytes = row[\"image\"][\"bytes\"]\n        image = Image.open(BytesIO(image_bytes)).convert(\"L\")\n\n        if self.transform:\n            image = self.transform(image)\n\n        # Tokenize report\n        encoded_text = self.tokenizer(\n            row[\"report\"],\n            padding=\"max_length\",\n            truncation=True,\n            max_length=144,\n            return_tensors=\"pt\"\n        )\n\n        input_ids = encoded_text[\"input_ids\"].squeeze(0)\n        attention_mask = encoded_text[\"attention_mask\"].squeeze(0)\n\n        return image, input_ids, attention_mask\n\n# tf (transform) and tokenizer defined above\ntest_set = HuggingFaceChestXRayDataset(df, tokenizer, tf)\n\n# Test a sample\nimage, input_ids, attention_mask = test_set[0]\n\nprint(\"Image shape:\", image.shape)\nprint(\"Input IDs:\", input_ids.shape)\nprint(\"Attention Mask:\", attention_mask.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T20:53:33.464748Z","iopub.execute_input":"2025-05-14T20:53:33.465016Z","iopub.status.idle":"2025-05-14T20:53:33.480809Z","shell.execute_reply.started":"2025-05-14T20:53:33.464992Z","shell.execute_reply":"2025-05-14T20:53:33.480035Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# take a random row from the new dataset \nimage, input_ids, attention_mask = test_set[0]\n\n# display the new image\nplt.imshow(image.squeeze(0), cmap=\"gray\")\nplt.title(\"Chest X-Ray\")\nplt.axis(\"off\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T20:53:33.481644Z","iopub.execute_input":"2025-05-14T20:53:33.481967Z","iopub.status.idle":"2025-05-14T20:53:33.637951Z","shell.execute_reply.started":"2025-05-14T20:53:33.481937Z","shell.execute_reply":"2025-05-14T20:53:33.637172Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# generate a text from the test_set\ntest_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=True)\ngenerate_text_from_dataset(test_loader, final_model, tokenizer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T20:53:33.638758Z","iopub.execute_input":"2025-05-14T20:53:33.639047Z","iopub.status.idle":"2025-05-14T20:53:36.936505Z","shell.execute_reply.started":"2025-05-14T20:53:33.639024Z","shell.execute_reply":"2025-05-14T20:53:36.935742Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **Metrics application**\n\n- Here we apply some metrics (**ROUGE** and **BLEU**) to our model to analyze its behavior","metadata":{}},{"cell_type":"code","source":"!pip install rouge-score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T20:53:36.937436Z","iopub.execute_input":"2025-05-14T20:53:36.937774Z","iopub.status.idle":"2025-05-14T20:53:42.252632Z","shell.execute_reply.started":"2025-05-14T20:53:36.937743Z","shell.execute_reply":"2025-05-14T20:53:42.251714Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\nfrom rouge_score import rouge_scorer\nimport random","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T20:53:42.253973Z","iopub.execute_input":"2025-05-14T20:53:42.254317Z","iopub.status.idle":"2025-05-14T20:53:42.763240Z","shell.execute_reply.started":"2025-05-14T20:53:42.254283Z","shell.execute_reply":"2025-05-14T20:53:42.762552Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def calculate_rouge(prediction, reference):\n    scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeL\"], use_stemmer=True)\n    scores = scorer.score(reference, prediction)\n    return scores['rouge1'].fmeasure, scores['rouge2'].fmeasure, scores['rougeL'].fmeasure\n\n\ndef calculate_metrics(val_set, modell, sample_ratio=0.3):\n    sample_size = int(len(val_set) * sample_ratio)\n    \n    sample_indices = random.sample(range(len(val_set)), sample_size)\n    \n    # Subset\n    val_sample = Subset(val_set, sample_indices)\n    \n    # We create a new dataloader\n    val_sample_loader = DataLoader(val_sample, batch_size=1, shuffle=False)\n    \n    bleu_scores = []\n    \n    rouge1_scores = []\n    rouge2_scores = []\n    rougeL_scores = []\n    \n    for images, input_ids, attention_mask in val_sample_loader:\n        images = images.to(device)\n        input_ids = input_ids.to(device)\n        attention_mask = attention_mask.to(device)\n    \n        with torch.no_grad():\n            predicted_text = modell(images, attention_mask).to(device)\n    \n        pred_text = tokenizer.decode(predicted_text[0], skip_special_tokens=True)\n        true_text = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n    \n        # We calculate bleu \n        bleu_result = sentence_bleu([true_text.split()], pred_text.split())\n    \n        bleu_scores.append(bleu_result)\n    \n        # We calculate rouge\n        rouge1, rouge2, rougeL = calculate_rouge(true_text, pred_text)\n        rouge1_scores.append(rouge1)\n        rouge2_scores.append(rouge2)\n        rougeL_scores.append(rougeL)\n\n    return bleu_scores, rouge1_scores, rouge2_scores, rougeL_scores\n\nbleu_scores, rouge1_scores, rouge2_scores, rougeL_scores = calculate_metrics(test_set, final_model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T20:53:42.763968Z","iopub.execute_input":"2025-05-14T20:53:42.764195Z","iopub.status.idle":"2025-05-14T21:07:14.499602Z","shell.execute_reply.started":"2025-05-14T20:53:42.764174Z","shell.execute_reply":"2025-05-14T21:07:14.498943Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### **Plot of BLEU metric**","metadata":{}},{"cell_type":"code","source":"def plot_bleu(scores, save_path):\n    mean_bleu = np.mean(scores)\n    print(f\"Mean BLEU on sampling: {mean_bleu:.4f}\")\n    \n    # Histogram\n    plt.figure(figsize=(12, 8))\n    plt.hist(scores, bins=20, range=(0, 1), color='skyblue', edgecolor='black')\n    plt.title('BLEU scores distribution')\n    plt.xlabel('BLEU score')\n    plt.ylabel('Frequency')\n    plt.xticks(np.linspace(0, 1, 21))  # Tick every 0.05, 21 values\n    plt.grid(True)\n    plt.savefig(f\"{save_path}_bleu.png\")\n    plt.show()\n\nsave_path = \"metrics_analysis\"\nplot_bleu(bleu_scores, save_path=save_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T21:07:14.500525Z","iopub.execute_input":"2025-05-14T21:07:14.500842Z","iopub.status.idle":"2025-05-14T21:07:14.914668Z","shell.execute_reply.started":"2025-05-14T21:07:14.500794Z","shell.execute_reply":"2025-05-14T21:07:14.913632Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### **Plot of ROUGE metric**","metadata":{}},{"cell_type":"code","source":"def plot_rouge(rouge1, rouge2, rougeL, save_path):\n    mean_rouge1 = np.mean(rouge1)\n    mean_rouge2 = np.mean(rouge2)\n    mean_rougeL = np.mean(rougeL)\n    \n    print(f\"Mean ROUGE-1: {mean_rouge1:.4f}\")\n    print(f\"Mean ROUGE-2: {mean_rouge2:.4f}\")\n    print(f\"Mean ROUGE-L: {mean_rougeL:.4f}\")\n    \n    # Histogram for ROUGE-1\n    plt.figure(figsize=(12, 8))\n    plt.hist(rouge1, bins=20, range=(0, 1), color='lightcoral', edgecolor='black')\n    plt.title('Distribuzione ROUGE-1 scores')\n    plt.xlabel('ROUGE-1 score (0 → 1)')\n    plt.ylabel('Frequency')\n    plt.xticks(np.linspace(0, 1, 21))  # Tick every 0.05, 21 values\n    plt.grid(True)\n    plt.savefig(f\"{save_path}_rouge1.png\")\n    plt.show()\n    \n    # Histogram for ROUGE-2\n    plt.figure(figsize=(12, 8))\n    plt.hist(rouge2, bins=20, range=(0, 1), color='lightgreen', edgecolor='black')\n    plt.title('ROUGE-2 scores distribution')\n    plt.xlabel('ROUGE-2 score (0 → 1)')\n    plt.ylabel('Frequency')\n    plt.xticks(np.linspace(0, 1, 21))  # Tick every 0.05, 21 values\n    plt.grid(True)\n    plt.savefig(f\"{save_path}_rouge2.png\")\n    plt.show()\n    \n    # Histogram for ROUGE-L\n    plt.figure(figsize=(12, 8))\n    plt.hist(rougeL, bins=20, range=(0, 1), color='lightblue', edgecolor='black')\n    plt.title('ROUGE-L scores distribution')\n    plt.xlabel('ROUGE-L score (0 → 1)')\n    plt.ylabel('Frequency')\n    plt.xticks(np.linspace(0, 1, 21))  # Tick every 0.05, 21 values\n    plt.grid(True)\n    plt.savefig(f\"{save_path}_rougeL.png\")\n    plt.show()\n\nplot_rouge(rouge1_scores, rouge2_scores, rougeL_scores, save_path=save_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T21:07:14.915636Z","iopub.execute_input":"2025-05-14T21:07:14.915977Z","iopub.status.idle":"2025-05-14T21:07:16.113600Z","shell.execute_reply.started":"2025-05-14T21:07:14.915941Z","shell.execute_reply":"2025-05-14T21:07:16.112726Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### **We build the model using BioGPT as transformer**\n- In order to calculate the metrics for both GPT2 and BioGPT  ","metadata":{}},{"cell_type":"code","source":"# for BioGPT tokenizer\n!pip install sacremoses","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T21:07:16.114506Z","iopub.execute_input":"2025-05-14T21:07:16.114849Z","iopub.status.idle":"2025-05-14T21:07:19.759026Z","shell.execute_reply.started":"2025-05-14T21:07:16.114789Z","shell.execute_reply":"2025-05-14T21:07:19.757957Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# initialize the tokenizer\nbiogpt_tokenizer = BioGptTokenizer.from_pretrained(\"microsoft/biogpt\") \nbiogpt_tokenizer.pad_token = biogpt_tokenizer.eos_token\n\n# initialize the model\nbiogpt = BioGptForCausalLM.from_pretrained(\"microsoft/biogpt\").to(device)\nbiogpt_hidden_size = biogpt.config.hidden_size\n\n# initialize the dataset\nbiogpt_test_set = HuggingFaceChestXRayDataset(df, biogpt_tokenizer, tf)\n\n# initialize the dataLoader\nbiogpt_test_loader = DataLoader(biogpt_test_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)\n\n# import the biogpt ff_mapper\nbiogpt_mapper = FF_mapper(512, biogpt_hidden_size).to(device)\nbiogpt_mapper.load_state_dict(torch.load(\"/kaggle/input/ff_mapper_biogpt/pytorch/default/1/ff_mapper_BioGPT.pth\"))\nbiogpt_mapper.to(device)\n\n# initialize the custom autoencoder with biogpt_mapper and BioGPT \nbiogpt_autoencoder_model = CustomAutoencoder(encoder, biogpt_mapper, biogpt)\n\n# generate a text from a dataset row\ngenerate_text_from_dataset(biogpt_test_loader, biogpt_autoencoder_model, biogpt_tokenizer)\nbiogpt_save_path = \"metrics_analysis_biogpt\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T21:33:03.531697Z","iopub.execute_input":"2025-05-14T21:33:03.532058Z","iopub.status.idle":"2025-05-14T21:33:13.539625Z","shell.execute_reply.started":"2025-05-14T21:33:03.532018Z","shell.execute_reply":"2025-05-14T21:33:13.537921Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### **Checking metrics on the BioGPT model**","metadata":{}},{"cell_type":"code","source":"biogpt_bleu_scores, biogpt_rouge1_scores, biogpt_rouge2_scores, biogpt_rougeL_scores = calculate_metrics(biogpt_test_set, biogpt_autoencoder_model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T21:07:38.361482Z","iopub.execute_input":"2025-05-14T21:07:38.362171Z","iopub.status.idle":"2025-05-14T21:28:16.422248Z","shell.execute_reply.started":"2025-05-14T21:07:38.362141Z","shell.execute_reply":"2025-05-14T21:28:16.421251Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_bleu(biogpt_bleu_scores, save_path=biogpt_save_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T21:33:18.120341Z","iopub.execute_input":"2025-05-14T21:33:18.120672Z","iopub.status.idle":"2025-05-14T21:33:18.966200Z","shell.execute_reply.started":"2025-05-14T21:33:18.120642Z","shell.execute_reply":"2025-05-14T21:33:18.965504Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_rouge(biogpt_rouge1_scores, biogpt_rouge2_scores, biogpt_rougeL_scores, save_path=biogpt_save_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T21:33:23.933267Z","iopub.execute_input":"2025-05-14T21:33:23.933643Z","iopub.status.idle":"2025-05-14T21:33:25.159662Z","shell.execute_reply.started":"2025-05-14T21:33:23.933611Z","shell.execute_reply":"2025-05-14T21:33:25.158837Z"}},"outputs":[],"execution_count":null}]}