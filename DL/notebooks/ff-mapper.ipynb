{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":951996,"sourceType":"datasetVersion","datasetId":516716},{"sourceId":301853,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":257787,"modelId":279061}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"onColab = False\n\nif onColab:\n    ! pip install kaggle\n    ! mkdir ~/.kaggle\n    ! cp kaggle.json ~/.kaggle/\n    ! chmod 600 ~/.kaggle/kaggle.json","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T21:21:18.431230Z","iopub.execute_input":"2025-05-04T21:21:18.431600Z","iopub.status.idle":"2025-05-04T21:21:18.438959Z","shell.execute_reply.started":"2025-05-04T21:21:18.431562Z","shell.execute_reply":"2025-05-04T21:21:18.437965Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"if onColab:\n    ! kaggle datasets download raddar/chest-xrays-indiana-university","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T21:21:18.440478Z","iopub.execute_input":"2025-05-04T21:21:18.440883Z","iopub.status.idle":"2025-05-04T21:21:18.455866Z","shell.execute_reply.started":"2025-05-04T21:21:18.440852Z","shell.execute_reply":"2025-05-04T21:21:18.455048Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"import zipfile\nimport os\n\nif onColab:\n    file_name = \"chest-xrays-indiana-university.zip\"\n    \n    # extract the file from the zip\n    with zipfile.ZipFile(file_name, 'r') as zip_ref:\n        zip_ref.extractall(\"chest_xrays_data\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T21:21:18.457193Z","iopub.execute_input":"2025-05-04T21:21:18.457416Z","iopub.status.idle":"2025-05-04T21:21:18.469955Z","shell.execute_reply.started":"2025-05-04T21:21:18.457397Z","shell.execute_reply":"2025-05-04T21:21:18.469116Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"if onColab:\n    !ls chest_xrays_data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T21:21:18.471104Z","iopub.execute_input":"2025-05-04T21:21:18.471370Z","iopub.status.idle":"2025-05-04T21:21:18.486069Z","shell.execute_reply.started":"2025-05-04T21:21:18.471350Z","shell.execute_reply":"2025-05-04T21:21:18.485244Z"}},"outputs":[],"execution_count":38},{"cell_type":"code","source":"if onColab: \n    img_dir = 'chest_xrays_data/images/images_normalized/'\n    reports_dir = 'chest_xrays_data/indiana_reports.csv'\n    projections_dir = 'chest_xrays_data/indiana_projections.csv'\nelse:\n    img_dir = '/kaggle/input/chest-xrays-indiana-university/images/images_normalized/'\n    reports_dir = '/kaggle/input/chest-xrays-indiana-university/indiana_reports.csv'\n    projections_dir = '/kaggle/input/chest-xrays-indiana-university/indiana_projections.csv'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T21:21:18.486925Z","iopub.execute_input":"2025-05-04T21:21:18.487198Z","iopub.status.idle":"2025-05-04T21:21:18.498899Z","shell.execute_reply.started":"2025-05-04T21:21:18.487160Z","shell.execute_reply":"2025-05-04T21:21:18.498122Z"}},"outputs":[],"execution_count":39},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-04T21:21:18.499715Z","iopub.execute_input":"2025-05-04T21:21:18.499982Z","iopub.status.idle":"2025-05-04T21:21:18.512914Z","shell.execute_reply.started":"2025-05-04T21:21:18.499955Z","shell.execute_reply":"2025-05-04T21:21:18.512192Z"}},"outputs":[],"execution_count":40},{"cell_type":"code","source":"from PIL import Image\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T21:21:18.513691Z","iopub.execute_input":"2025-05-04T21:21:18.513940Z","iopub.status.idle":"2025-05-04T21:21:18.526769Z","shell.execute_reply.started":"2025-05-04T21:21:18.513921Z","shell.execute_reply":"2025-05-04T21:21:18.525987Z"}},"outputs":[],"execution_count":41},{"cell_type":"code","source":"import torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import GPT2Tokenizer, GPT2LMHeadModel, BioGptTokenizer, BioGptForCausalLM\n\nfrom tqdm import tqdm\nfrom tqdm.auto import trange\n\nimport torchvision\nfrom torchvision import transforms as T","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T21:21:18.528423Z","iopub.execute_input":"2025-05-04T21:21:18.528615Z","iopub.status.idle":"2025-05-04T21:21:18.540623Z","shell.execute_reply.started":"2025-05-04T21:21:18.528598Z","shell.execute_reply":"2025-05-04T21:21:18.539968Z"}},"outputs":[],"execution_count":42},{"cell_type":"code","source":"# for BioGPT tokenizer\n!pip install sacremoses","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T21:21:18.541394Z","iopub.execute_input":"2025-05-04T21:21:18.541576Z","iopub.status.idle":"2025-05-04T21:21:21.881385Z","shell.execute_reply.started":"2025-05-04T21:21:18.541560Z","shell.execute_reply":"2025-05-04T21:21:21.880272Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: sacremoses in /usr/local/lib/python3.10/dist-packages (0.1.1)\nRequirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacremoses) (2024.11.6)\nRequirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sacremoses) (8.1.7)\nRequirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sacremoses) (1.4.2)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sacremoses) (4.67.1)\n","output_type":"stream"}],"execution_count":43},{"cell_type":"code","source":"torch.backends.cudnn.benchmark = True\n\ndevice = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n\nprint(f\"Using device: {torch.cuda.get_device_name(0)}\" if torch.cuda.is_available() else \"Using CPU\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T21:21:21.882760Z","iopub.execute_input":"2025-05-04T21:21:21.883103Z","iopub.status.idle":"2025-05-04T21:21:21.888358Z","shell.execute_reply.started":"2025-05-04T21:21:21.883071Z","shell.execute_reply":"2025-05-04T21:21:21.887527Z"}},"outputs":[{"name":"stdout","text":"Using device: Tesla P100-PCIE-16GB\n","output_type":"stream"}],"execution_count":44},{"cell_type":"markdown","source":"#### **Preprocessing**","metadata":{}},{"cell_type":"code","source":"reports_df = pd.read_csv(reports_dir)\nreports_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T21:21:21.890190Z","iopub.execute_input":"2025-05-04T21:21:21.890515Z","iopub.status.idle":"2025-05-04T21:21:21.941209Z","shell.execute_reply.started":"2025-05-04T21:21:21.890493Z","shell.execute_reply":"2025-05-04T21:21:21.940358Z"}},"outputs":[{"execution_count":45,"output_type":"execute_result","data":{"text/plain":"   uid                                               MeSH  \\\n0    1                                             normal   \n1    2  Cardiomegaly/borderline;Pulmonary Artery/enlarged   \n2    3                                             normal   \n3    4  Pulmonary Disease, Chronic Obstructive;Bullous...   \n4    5  Osteophyte/thoracic vertebrae/multiple/small;T...   \n\n                                            Problems  \\\n0                                             normal   \n1                      Cardiomegaly;Pulmonary Artery   \n2                                             normal   \n3  Pulmonary Disease, Chronic Obstructive;Bullous...   \n4                         Osteophyte;Thickening;Lung   \n\n                                               image  \\\n0                          Xray Chest PA and Lateral   \n1                Chest, 2 views, frontal and lateral   \n2                          Xray Chest PA and Lateral   \n3  PA and lateral views of the chest XXXX, XXXX a...   \n4                          Xray Chest PA and Lateral   \n\n                                          indication      comparison  \\\n0                                   Positive TB test           None.   \n1                           Preop bariatric surgery.           None.   \n2  rib pain after a XXXX, XXXX XXXX steps this XX...             NaN   \n3                      XXXX-year-old XXXX with XXXX.  None available   \n4                        Chest and nasal congestion.             NaN   \n\n                                            findings  \\\n0  The cardiac silhouette and mediastinum size ar...   \n1  Borderline cardiomegaly. Midline sternotomy XX...   \n2                                                NaN   \n3  There are diffuse bilateral interstitial and a...   \n4  The cardiomediastinal silhouette and pulmonary...   \n\n                                          impression  \n0                               Normal chest x-XXXX.  \n1                       No acute pulmonary findings.  \n2  No displaced rib fractures, pneumothorax, or p...  \n3  1. Bullous emphysema and interstitial fibrosis...  \n4              No acute cardiopulmonary abnormality.  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>uid</th>\n      <th>MeSH</th>\n      <th>Problems</th>\n      <th>image</th>\n      <th>indication</th>\n      <th>comparison</th>\n      <th>findings</th>\n      <th>impression</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>normal</td>\n      <td>normal</td>\n      <td>Xray Chest PA and Lateral</td>\n      <td>Positive TB test</td>\n      <td>None.</td>\n      <td>The cardiac silhouette and mediastinum size ar...</td>\n      <td>Normal chest x-XXXX.</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>Cardiomegaly/borderline;Pulmonary Artery/enlarged</td>\n      <td>Cardiomegaly;Pulmonary Artery</td>\n      <td>Chest, 2 views, frontal and lateral</td>\n      <td>Preop bariatric surgery.</td>\n      <td>None.</td>\n      <td>Borderline cardiomegaly. Midline sternotomy XX...</td>\n      <td>No acute pulmonary findings.</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>normal</td>\n      <td>normal</td>\n      <td>Xray Chest PA and Lateral</td>\n      <td>rib pain after a XXXX, XXXX XXXX steps this XX...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>No displaced rib fractures, pneumothorax, or p...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>Pulmonary Disease, Chronic Obstructive;Bullous...</td>\n      <td>Pulmonary Disease, Chronic Obstructive;Bullous...</td>\n      <td>PA and lateral views of the chest XXXX, XXXX a...</td>\n      <td>XXXX-year-old XXXX with XXXX.</td>\n      <td>None available</td>\n      <td>There are diffuse bilateral interstitial and a...</td>\n      <td>1. Bullous emphysema and interstitial fibrosis...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>Osteophyte/thoracic vertebrae/multiple/small;T...</td>\n      <td>Osteophyte;Thickening;Lung</td>\n      <td>Xray Chest PA and Lateral</td>\n      <td>Chest and nasal congestion.</td>\n      <td>NaN</td>\n      <td>The cardiomediastinal silhouette and pulmonary...</td>\n      <td>No acute cardiopulmonary abnormality.</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":45},{"cell_type":"code","source":"projections_df = pd.read_csv(projections_dir)\nprojections_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T21:21:21.942356Z","iopub.execute_input":"2025-05-04T21:21:21.942576Z","iopub.status.idle":"2025-05-04T21:21:21.960015Z","shell.execute_reply.started":"2025-05-04T21:21:21.942557Z","shell.execute_reply":"2025-05-04T21:21:21.959220Z"}},"outputs":[{"execution_count":46,"output_type":"execute_result","data":{"text/plain":"   uid                filename projection\n0    1  1_IM-0001-4001.dcm.png    Frontal\n1    1  1_IM-0001-3001.dcm.png    Lateral\n2    2  2_IM-0652-1001.dcm.png    Frontal\n3    2  2_IM-0652-2001.dcm.png    Lateral\n4    3  3_IM-1384-1001.dcm.png    Frontal","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>uid</th>\n      <th>filename</th>\n      <th>projection</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>1_IM-0001-4001.dcm.png</td>\n      <td>Frontal</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>1_IM-0001-3001.dcm.png</td>\n      <td>Lateral</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>2_IM-0652-1001.dcm.png</td>\n      <td>Frontal</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2</td>\n      <td>2_IM-0652-2001.dcm.png</td>\n      <td>Lateral</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>3</td>\n      <td>3_IM-1384-1001.dcm.png</td>\n      <td>Frontal</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":46},{"cell_type":"code","source":"# filter the rows with null findings\nreports_filtered = reports_df.dropna(subset=[\"findings\"])\n\n# keep only entries in projections that have a filtered report associated (association through uid)\nprojections_filtered = projections_df[projections_df[\"uid\"].isin(reports_filtered[\"uid\"])]\nreports_filtered.shape, projections_filtered.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T21:21:21.960866Z","iopub.execute_input":"2025-05-04T21:21:21.961105Z","iopub.status.idle":"2025-05-04T21:21:21.973416Z","shell.execute_reply.started":"2025-05-04T21:21:21.961081Z","shell.execute_reply":"2025-05-04T21:21:21.972722Z"}},"outputs":[{"execution_count":47,"output_type":"execute_result","data":{"text/plain":"((3337, 8), (6469, 3))"},"metadata":{}}],"execution_count":47},{"cell_type":"code","source":"VAL_SIZE = 0.1\n\nuids = reports_filtered.uid.unique()\n\ntrain_ds, val_ds = train_test_split(\n    uids,\n    test_size=VAL_SIZE,\n    random_state=42\n)\n\nlen(train_ds), len(val_ds)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T21:21:21.974241Z","iopub.execute_input":"2025-05-04T21:21:21.974487Z","iopub.status.idle":"2025-05-04T21:21:21.984640Z","shell.execute_reply.started":"2025-05-04T21:21:21.974468Z","shell.execute_reply":"2025-05-04T21:21:21.983830Z"}},"outputs":[{"execution_count":48,"output_type":"execute_result","data":{"text/plain":"(3003, 334)"},"metadata":{}}],"execution_count":48},{"cell_type":"markdown","source":"#### **Load the pre-trained transformer and build the dataset**","metadata":{}},{"cell_type":"code","source":"def load_model_and_tokenizer(model_name=\"gpt2\"): \n    if model_name == \"BioGPT\":\n        tokenizer = BioGptTokenizer.from_pretrained(\"microsoft/biogpt\") \n        tokenizer.pad_token = tokenizer.eos_token\n        model = BioGptForCausalLM.from_pretrained(\"microsoft/biogpt\").to(device)\n        hidden_size = model.config.hidden_size\n    else:\n        tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n        tokenizer.pad_token = tokenizer.eos_token\n        model = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(device)\n        hidden_size = model.config.n_embd\n        \n    for param in model.parameters():\n        param.requires_grad = True  # Freezes all transformer parameters\n\n    transformer_parameters= sum(p.numel() for p in model.parameters())\n    print(f\"Number of transformer parameters: {transformer_parameters}\")\n\n    return tokenizer, model, hidden_size\n\n# you need to change only this variable to change the transformer!!\nmodel_name = \"BioGPT\"\ntokenizer, transformerModel, hidden_size = load_model_and_tokenizer(model_name)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T21:21:21.985509Z","iopub.execute_input":"2025-05-04T21:21:21.985785Z","iopub.status.idle":"2025-05-04T21:21:23.337515Z","shell.execute_reply.started":"2025-05-04T21:21:21.985752Z","shell.execute_reply":"2025-05-04T21:21:23.336281Z"}},"outputs":[{"name":"stdout","text":"Number of transformer parameters: 346763264\n","output_type":"stream"}],"execution_count":49},{"cell_type":"code","source":"# adjusted dataset\nclass ChestXRayDataset(Dataset):\n    def __init__(self, reports_df, projections_df, image_folder, tokenizer, uids, transforms):\n        self.reports_df = reports_df[reports_df[\"uid\"].isin(uids)].reset_index(drop=True)\n        self.projections_df = projections_df\n        self.image_folder = image_folder\n        self.tokenizer = tokenizer\n        # a series of transformations to be applied to images before feeding them into a model\n        self.transform = transforms\n\n    def __len__(self):\n        return len(self.reports_df)\n\n    def __getitem__(self, idx):\n        row = self.reports_df.iloc[idx]\n        uid = row[\"uid\"]\n        text = row[\"findings\"]\n\n        # tokenize findings column\n        encoded_text = self.tokenizer(\n            text,\n            padding=\"max_length\",\n            truncation=True,\n            max_length=144,\n            return_tensors=\"pt\"\n        )\n\n        # find the path and filename of the associated image\n        image_filename = self.projections_df[self.projections_df[\"uid\"] == uid][\"filename\"].values[0]\n        image_path = f\"{self.image_folder}/{image_filename}\"\n\n        # load and trasform the image\n        image = Image.open(image_path).convert(\"L\")  # conversion to grayscale\n        image = self.transform(image)\n\n        # return the image, label (finding)\n        return image, encoded_text[\"input_ids\"].squeeze(0), encoded_text[\"attention_mask\"].squeeze(0)\n\ntf = T.Compose([\n    T.Resize((224, 224)),  # resizing for pre-trained models\n    T.ToTensor(),\n])\n\ntrain_dataset = ChestXRayDataset(reports_filtered, projections_filtered, img_dir, tokenizer, train_ds, tf)\nval_dataset = ChestXRayDataset(reports_filtered, projections_filtered, img_dir, tokenizer, val_ds, tf)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T21:21:23.340623Z","iopub.execute_input":"2025-05-04T21:21:23.340927Z","iopub.status.idle":"2025-05-04T21:21:23.356827Z","shell.execute_reply.started":"2025-05-04T21:21:23.340899Z","shell.execute_reply":"2025-05-04T21:21:23.356072Z"}},"outputs":[],"execution_count":50},{"cell_type":"code","source":"BATCH_SIZE = 16\n\n# create the DataLoader to generate batches of the dataset and iterate over them\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)\nval_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, num_workers=4, pin_memory=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T21:21:23.358747Z","iopub.execute_input":"2025-05-04T21:21:23.359056Z","iopub.status.idle":"2025-05-04T21:21:23.371492Z","shell.execute_reply.started":"2025-05-04T21:21:23.359021Z","shell.execute_reply":"2025-05-04T21:21:23.370741Z"}},"outputs":[],"execution_count":51},{"cell_type":"code","source":"def conv_layer(n_input, n_output, kernel_size, stride=1):\n    return nn.Sequential(\n        nn.Conv2d(n_input, n_output, kernel_size, stride),\n        nn.ReLU(),\n        nn.BatchNorm2d(n_output),\n        nn.MaxPool2d(2)\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T21:21:23.372370Z","iopub.execute_input":"2025-05-04T21:21:23.372644Z","iopub.status.idle":"2025-05-04T21:21:23.386057Z","shell.execute_reply.started":"2025-05-04T21:21:23.372615Z","shell.execute_reply":"2025-05-04T21:21:23.385269Z"}},"outputs":[],"execution_count":52},{"cell_type":"code","source":"encoder = nn.Sequential(\n            conv_layer(1, 64, 3),\n            conv_layer(64, 128, 3),\n            conv_layer(128, 256, 3),\n            conv_layer(256, 512, 3)\n        )\n\nencoder.load_state_dict(torch.load(\"/kaggle/input/encodercnn/pytorch/default/1/encoder.pth\"))\nencoder.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T21:21:23.386928Z","iopub.execute_input":"2025-05-04T21:21:23.387177Z","iopub.status.idle":"2025-05-04T21:21:23.441550Z","shell.execute_reply.started":"2025-05-04T21:21:23.387152Z","shell.execute_reply":"2025-05-04T21:21:23.440886Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-53-3b924056fbb6>:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  encoder.load_state_dict(torch.load(\"/kaggle/input/encodercnn/pytorch/default/1/encoder.pth\"))\n","output_type":"stream"},{"execution_count":53,"output_type":"execute_result","data":{"text/plain":"Sequential(\n  (0): Sequential(\n    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1))\n    (1): ReLU()\n    (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (1): Sequential(\n    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n    (1): ReLU()\n    (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (2): Sequential(\n    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))\n    (1): ReLU()\n    (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (3): Sequential(\n    (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1))\n    (1): ReLU()\n    (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n)"},"metadata":{}}],"execution_count":53},{"cell_type":"markdown","source":"#### **Visualize the latent space generated**","metadata":{}},{"cell_type":"code","source":"data_iter = iter(val_loader)\ninputs, _, _ = next(data_iter)\n\ninputs = inputs.to(device)\n\nwith torch.no_grad():\n    latent_space = encoder(inputs)\n\ninputs = inputs.cpu().numpy()\nlatent_space = latent_space.cpu().numpy()\n\nfor idx in range(2):\n    reconstructed = latent_space[idx, 0]\n\n    print(f\"{idx+1}) Latent Space (dim={len(latent_space[idx, 0])}) -> {reconstructed}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T21:21:23.442321Z","iopub.execute_input":"2025-05-04T21:21:23.442580Z","iopub.status.idle":"2025-05-04T21:21:24.821461Z","shell.execute_reply.started":"2025-05-04T21:21:23.442549Z","shell.execute_reply":"2025-05-04T21:21:24.820019Z"}},"outputs":[{"name":"stdout","text":"1) Latent Space (dim=12) -> [[ 2.3342481e+00  7.4157872e+00  6.0161729e+00  4.6807127e+00\n  -2.1490312e-01 -2.1490312e-01 -2.1490312e-01 -2.1490312e-01\n  -2.1490312e-01  2.8534083e+00  6.5627036e+00  7.1844749e+00]\n [ 3.2660177e+00  2.3519361e+00  1.9366018e+00  7.5438648e-01\n  -2.1490312e-01 -2.1490312e-01 -2.1490312e-01  3.1695980e-01\n  -2.1490312e-01 -2.1490312e-01 -2.1490312e-01  5.0425382e+00]\n [-2.2234943e-02 -2.1490312e-01 -2.1490312e-01 -2.1490312e-01\n  -2.1490312e-01 -2.1490312e-01  2.7784839e+00 -1.7517197e-01\n  -2.1490312e-01 -2.1490312e-01  1.4653642e+00  1.7443779e+00]\n [-2.1490312e-01 -2.1490312e-01 -1.7824513e-01  1.9602889e+00\n  -2.1490312e-01  6.8956339e-03  4.6908006e-01 -1.3318159e-01\n  -2.1490312e-01 -2.1490312e-01 -8.7809145e-02  4.9835712e-01]\n [-2.1490312e-01 -2.1490312e-01  3.5161749e-01 -2.1490312e-01\n  -2.1490312e-01  1.2977669e+00  3.8835564e+00  1.8848686e+00\n  -2.1490312e-01 -2.1490312e-01 -6.6855192e-02 -2.1490312e-01]\n [-2.1490312e-01  9.4445586e-01  2.7238155e-02  1.9562241e+00\n  -2.1490312e-01  2.3752484e+00  2.6385865e+00  8.1106722e-01\n   2.9226911e-01 -2.1490312e-01 -2.1490312e-01 -2.1490312e-01]\n [-2.1490312e-01 -2.1490312e-01  4.2365804e-01  4.3461328e+00\n  -2.1490312e-01  2.3575249e-01  6.6581744e-01  6.7465061e-01\n  -2.1490312e-01 -2.1490312e-01 -2.1490312e-01 -2.1490312e-01]\n [-2.1490312e-01 -2.1490312e-01  5.7933168e+00  2.1593964e+00\n   2.6192577e+00 -1.6672815e-01 -2.1490312e-01  1.0454372e+00\n  -2.1490312e-01  1.4781791e+00 -2.1490312e-01 -2.1490312e-01]\n [-2.1490312e-01 -2.1490312e-01 -2.1490312e-01 -2.1490312e-01\n  -2.1490312e-01 -2.1490312e-01 -2.1490312e-01 -2.1490312e-01\n   5.5197424e-01 -2.1490312e-01 -2.1490312e-01 -2.1490312e-01]\n [-2.1490312e-01 -2.1490312e-01 -2.1490312e-01 -2.1490312e-01\n  -2.1490312e-01 -2.1490312e-01 -2.1490312e-01 -2.1490312e-01\n  -2.1490312e-01 -2.1490312e-01 -2.1490312e-01 -2.1490312e-01]\n [-2.1490312e-01 -2.1490312e-01 -2.1490312e-01 -2.1490312e-01\n  -2.1490312e-01 -2.1490312e-01 -2.1490312e-01 -2.1490312e-01\n  -2.1490312e-01 -2.1490312e-01 -2.1490312e-01 -2.1490312e-01]\n [-2.1490312e-01 -2.1490312e-01 -2.1490312e-01 -2.1490312e-01\n  -2.1490312e-01 -2.1490312e-01 -2.1490312e-01 -2.1490312e-01\n  -2.1490312e-01 -2.1490312e-01 -2.1490312e-01 -2.1490312e-01]]\n2) Latent Space (dim=12) -> [[ 1.6314820e+00 -2.1490312e-01 -2.1490312e-01 -2.1490312e-01\n  -2.1490312e-01 -2.1490312e-01 -2.1490312e-01 -2.1490312e-01\n   1.0995666e-03 -2.1490312e-01 -9.2702344e-02  8.7467444e-01]\n [-2.1490312e-01  3.6770231e-01 -2.1490312e-01 -2.1490312e-01\n  -2.1490312e-01 -2.1490312e-01 -2.1490312e-01 -2.1490312e-01\n  -2.1490312e-01 -2.1490312e-01 -2.1490312e-01  1.4527340e+00]\n [-2.1490312e-01 -2.1490312e-01 -1.9359620e-01 -2.1490312e-01\n  -2.1490312e-01 -2.1490312e-01 -2.1490312e-01 -2.1490312e-01\n  -2.1490312e-01 -2.1490312e-01 -2.1490312e-01 -2.1490312e-01]\n [-2.1490312e-01 -2.1490312e-01  8.7130255e-01  2.6788514e+00\n  -2.1490312e-01 -2.1490312e-01  2.4216778e+00  9.1581416e-01\n  -2.1490312e-01 -2.1490312e-01 -2.1490312e-01 -2.1490312e-01]\n [-2.1490312e-01 -2.1490312e-01  3.7534759e+00  1.7294891e-01\n  -2.1490312e-01 -2.1490312e-01  2.5664690e+00  5.2411213e+00\n  -2.1490312e-01 -2.1490312e-01 -2.1490312e-01 -2.1490312e-01]\n [-2.1490312e-01 -2.1490312e-01  1.3894470e+00  2.9889534e+00\n   2.5300705e+00  8.9271355e-01 -2.1490312e-01 -2.1490312e-01\n  -2.1490312e-01 -2.1490312e-01 -2.1490312e-01 -2.1490312e-01]\n [-2.1490312e-01 -2.1490312e-01  3.6008418e+00  3.4491386e+00\n   1.3099577e-01 -2.1490312e-01 -2.1490312e-01 -2.1490312e-01\n  -2.1490312e-01 -2.1490312e-01 -2.1490312e-01 -2.1490312e-01]\n [-2.1490312e-01 -2.1490312e-01  1.6006274e+00  5.8353680e-01\n  -2.1490312e-01 -2.1490312e-01 -2.1490312e-01 -2.1490312e-01\n  -2.1490312e-01 -2.1490312e-01 -2.1490312e-01 -2.1490312e-01]\n [-2.1490312e-01 -2.1490312e-01  4.8990903e+00  7.9323487e+00\n   4.1890349e+00 -2.1490312e-01 -2.1490312e-01 -2.1490312e-01\n  -2.1490312e-01 -1.9477305e-01 -2.1490312e-01 -2.1490312e-01]\n [-2.1490312e-01 -2.1490312e-01 -2.1490312e-01 -2.1490312e-01\n  -2.1490312e-01 -2.1490312e-01 -2.1490312e-01 -2.1490312e-01\n  -2.1490312e-01 -2.1490312e-01 -2.1490312e-01 -2.1490312e-01]\n [-2.1490312e-01 -2.1490312e-01 -2.1490312e-01 -2.1490312e-01\n  -2.1490312e-01 -2.1490312e-01 -2.1490312e-01 -2.1490312e-01\n  -2.1490312e-01 -2.1490312e-01 -2.1490312e-01 -2.1490312e-01]\n [-2.1490312e-01 -2.1490312e-01 -2.1490312e-01 -2.1490312e-01\n  -2.1490312e-01 -2.1490312e-01 -2.1490312e-01 -2.1490312e-01\n  -2.1490312e-01  6.4427537e-01  4.1618786e+00 -2.1490312e-01]]\n","output_type":"stream"}],"execution_count":54},{"cell_type":"markdown","source":"### **Build and train the FF mapper model**","metadata":{}},{"cell_type":"code","source":"def linear_layer(dim_input, dim_output, drop_p=0.1, last=False):\n    layers = [nn.Linear(dim_input, dim_output)]\n    if not last:\n        layers.append(nn.ReLU())\n        layers.append(nn.Dropout(p=drop_p))\n    return nn.Sequential(*layers)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T21:21:24.823423Z","iopub.execute_input":"2025-05-04T21:21:24.825380Z","iopub.status.idle":"2025-05-04T21:21:24.836072Z","shell.execute_reply.started":"2025-05-04T21:21:24.825341Z","shell.execute_reply":"2025-05-04T21:21:24.832543Z"}},"outputs":[],"execution_count":55},{"cell_type":"code","source":"class FF_mapper(nn.Module):\n\n    def __init__(self, dim_input, dim_output):\n        super().__init__()\n        self.ff = nn.Sequential(\n            linear_layer(dim_input, 640),\n            linear_layer(640, 896),\n            #linear_layer(896, 1024),\n            linear_layer(896, dim_output, last=True),\n            nn.LayerNorm(dim_output)\n        )\n        \n\n    def forward(self, latent_space):\n        # flatter, permute and stuff\n        batch_size, C, H, W = latent_space.shape\n        latent_space = latent_space.permute(0, 2, 3, 1)  # (1, 12, 12, 512)\n        latent_space = latent_space.view(batch_size, H * W, C)  # (1, 144, 512)\n        return self.ff(latent_space)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T21:21:24.836769Z","iopub.execute_input":"2025-05-04T21:21:24.837072Z","iopub.status.idle":"2025-05-04T21:21:24.852457Z","shell.execute_reply.started":"2025-05-04T21:21:24.837034Z","shell.execute_reply":"2025-05-04T21:21:24.850883Z"}},"outputs":[],"execution_count":56},{"cell_type":"code","source":"def soft_generate(inputs_embeds, attention_mask, labels, temperature=1.0):\n    outputs = transformerModel(\n        inputs_embeds=inputs_embeds, \n        attention_mask=attention_mask,\n        labels=labels,\n        return_dict=True\n    )\n    logits = outputs.logits  # [batch, seq_len, vocab_size]\n    # Apply softmax with temperature to get differentiable probabilities\n    soft_tokens = nn.functional.softmax(logits / temperature, dim=-1)\n    return soft_tokens  # This is a differentiable approximation","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T21:21:24.853746Z","iopub.execute_input":"2025-05-04T21:21:24.854015Z","iopub.status.idle":"2025-05-04T21:21:24.877858Z","shell.execute_reply.started":"2025-05-04T21:21:24.853989Z","shell.execute_reply":"2025-05-04T21:21:24.876465Z"}},"outputs":[],"execution_count":57},{"cell_type":"code","source":"def train(train_x, val_x, model, epochs=10):\n    criterion = mse_cos_sim_loss\n    alpha = 0.0    # used in mixed loss\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n\n    history = []\n    log_template = \"\\nEpoch {ep:03d} train_loss: {t_loss:0.4f} val_loss: {val_loss:0.4f}\"\n\n    with tqdm(desc=\"epoch\", total=epochs) as pbar_outer:\n        for epoch in range(epochs):\n            #if epoch/epochs >= 0.5:\n            #    alpha = 0.2\n            \n            train_loss = fit_epoch(model, train_x, criterion, optimizer, alpha)\n            val_loss = eval_epoch(model, val_x, criterion, alpha)\n            print(\"loss: \", train_loss)\n\n            history.append((train_loss,val_loss))\n\n            pbar_outer.update(1)\n            tqdm.write(log_template.format(ep=epoch+1, t_loss=train_loss, val_loss=val_loss))            \n        \n    return history\n\ndef cosine_similarity_loss(predicted_embedding, target_embedding):\n    return 1 - nn.functional.cosine_similarity(predicted_embedding, target_embedding, dim=-1).mean()\n\ndef mse_cos_sim_loss(pred, true, alpha=0.05, rescale=10):\n    # try with a rescale factor of 10\n    return alpha * nn.functional.mse_loss(pred, true) + (1-alpha) * rescale * cosine_similarity_loss(pred, true)\n\ndef fit_epoch(model, train_x, criterion, optimizer, alpha):\n    running_loss = 0.0\n    processed_data = 0\n\n    # for epoch progress\n    old_progress = -0.1\n    new_progress = 0\n\n    for idx, (images, text, attention) in enumerate(train_x):\n\n        new_progress = idx/len(train_x)\n        if (new_progress-old_progress >= 0.1):\n            print(f\"Epoch progress: {new_progress*100}%\")\n            old_progress = new_progress\n\n        images = images.to(device)\n        text = text.to(device)\n        attention = attention.to(device)\n\n        optimizer.zero_grad()\n\n        # Get latent space representation\n        with torch.no_grad():\n            latent_space = encoder(images).to(device)\n\n        # FFNN generates transformer input embeddings\n        pred_embeds = model(latent_space)\n\n        # len(pred_embeds), out => 32\n        # len(pred_embeds[0]), out => 144\n        # len(pred_embeds[0][0]), out => 768\n        # pred_embeds.shape, out => [32, 144, 768] => [batch_size, text_dim, emb_dim]\n\n        # generating the logits\n        generated_tokens = soft_generate(pred_embeds, attention, text)\n\n        emb_layer = transformerModel.get_input_embeddings()\n        vocab_embeddings = emb_layer.weight\n        true_y = emb_layer(text)\n        pred_y = torch.matmul(generated_tokens, vocab_embeddings)\n\n        #if (idx == 5):\n        #    print(f\"real tokens:\\n{text}\")\n        #    print(f\"pred tokens:\\n{generated_text}\")\n\n        # print(f\"Real Text:\\n{tokenizer.decode(text[0], skip_special_tokens=True)}\")\n        # print(f\"Generated Text:\\n{tokenizer.decode(generated_text[0], skip_special_tokens=True)}\")\n\n        # Compute loss\n        loss = criterion(pred_y, true_y, alpha)\n\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item() * images.shape[0]\n        processed_data += images.shape[0]\n        \n    return running_loss / processed_data\n\ndef eval_epoch(model, val_x, criterion, alpha):\n    running_loss = 0.0\n    processed_data = 0\n    model.eval()\n\n    with torch.no_grad():\n        for images, text, attention in val_x:\n            \n            images = images.to(device)\n            text = text.to(device)\n            attention = attention.to(device)\n\n            # using imported models to create the data we need\n            latent_space = encoder(images).to(device)\n\n            pred_embeds = model(latent_space)\n        \n            # generating the logits\n            generated_tokens = soft_generate(pred_embeds, attention, text)\n\n            emb_layer = transformerModel.get_input_embeddings()\n            vocab_embeddings = emb_layer.weight\n            true_y = emb_layer(text)\n            pred_y = torch.matmul(generated_tokens, vocab_embeddings)\n            \n            # Compute loss\n            loss = criterion(pred_y, true_y, alpha)\n            \n            running_loss += loss.item() * images.shape[0]\n            processed_data += images.shape[0]\n    \n    return running_loss / processed_data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T21:21:24.881092Z","iopub.execute_input":"2025-05-04T21:21:24.881342Z","iopub.status.idle":"2025-05-04T21:21:24.907970Z","shell.execute_reply.started":"2025-05-04T21:21:24.881318Z","shell.execute_reply":"2025-05-04T21:21:24.905101Z"}},"outputs":[],"execution_count":58},{"cell_type":"code","source":"mapper = FF_mapper(512, hidden_size).to(device)    # hidden_size = 768 for GPT2 and 1024 for BioGPT\n\nmapper_parameters= sum(p.numel() for p in mapper.parameters())\nprint(f\"number of mapper parameters: {mapper_parameters}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T21:21:24.908584Z","iopub.execute_input":"2025-05-04T21:21:24.909011Z","iopub.status.idle":"2025-05-04T21:21:24.960128Z","shell.execute_reply.started":"2025-05-04T21:21:24.908981Z","shell.execute_reply":"2025-05-04T21:21:24.956918Z"}},"outputs":[{"name":"stdout","text":"number of mapper parameters: 1823232\n","output_type":"stream"}],"execution_count":59},{"cell_type":"code","source":"import time\n\nstart = time.time()\nhistory = train(train_loader, val_loader, mapper, epochs=20)\nprint(f\"Training duration: {(time.time() - start) / 60} (min)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T21:21:24.960959Z","iopub.execute_input":"2025-05-04T21:21:24.961404Z","iopub.status.idle":"2025-05-04T21:21:51.304801Z","shell.execute_reply.started":"2025-05-04T21:21:24.961366Z","shell.execute_reply":"2025-05-04T21:21:51.303219Z"}},"outputs":[{"name":"stderr","text":"epoch:   0%|          | 0/20 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch progress: 0.0%\nEpoch progress: 10.106382978723403%\n","output_type":"stream"},{"name":"stderr","text":"epoch:   0%|          | 0/20 [00:26<?, ?it/s]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-60-ef0c3ce262cf>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Training duration: {(time.time() - start) / 60} (min)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-58-ed38138daa50>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_x, val_x, model, epochs)\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0;31m#    alpha = 0.2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m             \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loss: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-58-ed38138daa50>\u001b[0m in \u001b[0;36mfit_epoch\u001b[0;34m(model, train_x, criterion, optimizer, alpha)\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    579\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m             )\n\u001b[0;32m--> 581\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    582\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    826\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":60},{"cell_type":"code","source":"train_loss, val_loss = zip(*history)\nplt.figure(figsize=(15,10))\nplt.plot(train_loss, label='Train loss')\nplt.plot(val_loss, label='Val loss')\nplt.legend(loc='best')\nplt.xlabel(\"epochs\")\nplt.ylabel(\"loss\")\nplt.plot();","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T21:21:51.305530Z","iopub.status.idle":"2025-05-04T21:21:51.305913Z","shell.execute_reply":"2025-05-04T21:21:51.305752Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def generate_text(inputs_embeds, attention_mask):\n    return transformerModel.generate(\n        inputs_embeds=inputs_embeds, \n        max_length=288,\n        attention_mask=attention_mask,\n        pad_token_id=tokenizer.eos_token_id,\n        no_repeat_ngram_size=2,   # avoid repetitions\n        #top_k=50,   # considers only the 50 most probable words\n        eos_token_id=None,\n        do_sample=False\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T21:21:51.306845Z","iopub.status.idle":"2025-05-04T21:21:51.307165Z","shell.execute_reply":"2025-05-04T21:21:51.307028Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data_iter = iter(train_loader)\nimage, text, attention = next(data_iter)\n\nprint(f\"Real Text:\\n{tokenizer.decode(text[0], skip_special_tokens=True)}\\n\\n\")\n\nimage = image.to(device)\ntext = text.to(device)\nattention = attention.to(device)\n\nwith torch.no_grad():\n    latent_space = encoder(image).to(device)\n    predicted_embedding = mapper(latent_space).to(device)    \n\npredicted_text = generate_text(predicted_embedding, attention)\n\nprint(f\"Predicted Text:\\n{tokenizer.decode(predicted_text[0], skip_special_tokens=True)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T21:23:27.233340Z","iopub.execute_input":"2025-05-04T21:23:27.233730Z","iopub.status.idle":"2025-05-04T21:23:32.525555Z","shell.execute_reply.started":"2025-05-04T21:23:27.233689Z","shell.execute_reply":"2025-05-04T21:23:32.524279Z"}},"outputs":[{"name":"stdout","text":"Real Text:\nApparent scarring within the lingula. Lungs are otherwise clear. No pleural effusions or pneumothoraces. Heart and mediastinum of normal size and contour.\n\n\nPredicted Text:\nThe Effect of Different Doses of Vitamin C on the Growth of A study was carried out to evaluate the effect of different doses of vitamin C (1, 2, 4, 8, 16, 32, 64, 128, 256, and 256 mg / kg body weight) on growth of rats. It was observed that the growth was significantly (How to Avoid the Risk of Inappropriate Use of Antibiotics in the Intensive Care Unit. A Systematic Review. Part I: The Use and Timing of Antibiotic Therapy. The Role of the Microbiome. An Overview of Current Evidence. What Works. How To Avoids the Risks of Overuse of Antimicrobial Therapy in Intensive care Unit Patients. \"Antibiotic\n","output_type":"stream"}],"execution_count":64},{"cell_type":"code","source":"torch.save(mapper.state_dict(), f\"ff_mapper_{model_name}.pth\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T21:25:42.924098Z","iopub.execute_input":"2025-05-04T21:25:42.924457Z","iopub.status.idle":"2025-05-04T21:25:42.944161Z","shell.execute_reply.started":"2025-05-04T21:25:42.924418Z","shell.execute_reply":"2025-05-04T21:25:42.943500Z"}},"outputs":[],"execution_count":65}]}