{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":951996,"sourceType":"datasetVersion","datasetId":516716},{"sourceId":301853,"sourceType":"modelInstanceVersion","modelInstanceId":257787,"modelId":279061}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"#### The ***Latent Space Mapper*** is a Feed-Forward Neural Network whose purpose is to map the latent space generated by the encoder to the embedding that the transformer needs in input in order to generate a valid medical report.\n\nThis notebook shows the **embedding approach**.","metadata":{}},{"cell_type":"code","source":"onColab = False\n\nif onColab:\n    ! pip install kaggle\n    ! mkdir ~/.kaggle\n    ! cp kaggle.json ~/.kaggle/\n    ! chmod 600 ~/.kaggle/kaggle.json","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if onColab:\n    ! kaggle datasets download raddar/chest-xrays-indiana-university","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import zipfile\nimport os\n\nif onColab:\n    file_name = \"chest-xrays-indiana-university.zip\"\n    \n    # extract the file from the zip\n    with zipfile.ZipFile(file_name, 'r') as zip_ref:\n        zip_ref.extractall(\"chest_xrays_data\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if onColab:\n    !ls chest_xrays_data","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if onColab: \n    img_dir = 'chest_xrays_data/images/images_normalized/'\n    reports_dir = 'chest_xrays_data/indiana_reports.csv'\n    projections_dir = 'chest_xrays_data/indiana_projections.csv'\nelse:\n    img_dir = '/kaggle/input/chest-xrays-indiana-university/images/images_normalized/'\n    reports_dir = '/kaggle/input/chest-xrays-indiana-university/indiana_reports.csv'\n    projections_dir = '/kaggle/input/chest-xrays-indiana-university/indiana_projections.csv'","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from PIL import Image\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import GPT2Tokenizer, GPT2LMHeadModel, BioGptTokenizer, BioGptForCausalLM\n\nfrom tqdm import tqdm\nfrom tqdm.auto import trange\n\nimport torchvision\nfrom torchvision import transforms as T","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# for BioGPT tokenizer\n!pip install sacremoses","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch.backends.cudnn.benchmark = True\n\ndevice = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n\nprint(f\"Using device: {torch.cuda.get_device_name(0)}\" if torch.cuda.is_available() else \"Using CPU\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### **Preprocessing**","metadata":{}},{"cell_type":"code","source":"reports_df = pd.read_csv(reports_dir)\nreports_df.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"projections_df = pd.read_csv(projections_dir)\nprojections_df.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# filter the rows with null findings\nreports_filtered = reports_df.dropna(subset=[\"findings\"])\n\n# keep only entries in projections that have a filtered report associated (association through uid)\nprojections_filtered = projections_df[projections_df[\"uid\"].isin(reports_filtered[\"uid\"])]\nreports_filtered.shape, projections_filtered.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"VAL_SIZE = 0.1\n\nuids = reports_filtered.uid.unique()\n\ntrain_ds, val_ds = train_test_split(\n    uids,\n    test_size=VAL_SIZE,\n    random_state=42\n)\n\nlen(train_ds), len(val_ds)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### **Load the pre-trained transformer and build the dataset**","metadata":{}},{"cell_type":"code","source":"def load_model_and_tokenizer(model_name=\"gpt2\"): \n    if model_name == \"BioGPT\":\n        tokenizer = BioGptTokenizer.from_pretrained(\"microsoft/biogpt\") \n        tokenizer.pad_token = tokenizer.eos_token\n        model = BioGptForCausalLM.from_pretrained(\"microsoft/biogpt\").to(device)\n        hidden_size = model.config.hidden_size\n    else:\n        tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n        tokenizer.pad_token = tokenizer.eos_token\n        model = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(device)\n        hidden_size = model.config.n_embd\n        \n    for param in model.parameters():\n        param.requires_grad = True  # Freezes all transformer parameters\n\n    transformer_parameters= sum(p.numel() for p in model.parameters())\n    print(f\"Number of transformer parameters: {transformer_parameters}\")\n\n    return tokenizer, model, hidden_size\n\n# you need to change only this variable to change the transformer!!\nmodel_name = \"BioGPT\"\ntokenizer, transformerModel, hidden_size = load_model_and_tokenizer(model_name)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# adjusted dataset\nclass ChestXRayDataset(Dataset):\n    def __init__(self, reports_df, projections_df, image_folder, tokenizer, uids, transforms):\n        self.reports_df = reports_df[reports_df[\"uid\"].isin(uids)].reset_index(drop=True)\n        self.projections_df = projections_df\n        self.image_folder = image_folder\n        self.tokenizer = tokenizer\n        # a series of transformations to be applied to images before feeding them into a model\n        self.transform = transforms\n\n    def __len__(self):\n        return len(self.reports_df)\n\n    def __getitem__(self, idx):\n        row = self.reports_df.iloc[idx]\n        uid = row[\"uid\"]\n        text = row[\"findings\"]\n\n        # tokenize findings column\n        encoded_text = self.tokenizer(\n            text,\n            padding=\"max_length\",\n            truncation=True,\n            max_length=144,\n            return_tensors=\"pt\"\n        )\n\n        # find the path and filename of the associated image\n        image_filename = self.projections_df[self.projections_df[\"uid\"] == uid][\"filename\"].values[0]\n        image_path = f\"{self.image_folder}/{image_filename}\"\n\n        # load and trasform the image\n        image = Image.open(image_path).convert(\"L\")  # conversion to grayscale\n        image = self.transform(image)\n\n        # return the image, label (finding)\n        return image, encoded_text[\"input_ids\"].squeeze(0), encoded_text[\"attention_mask\"].squeeze(0)\n\ntf = T.Compose([\n    T.Resize((224, 224)),  # resizing for pre-trained models\n    T.ToTensor(),\n])\n\ntrain_dataset = ChestXRayDataset(reports_filtered, projections_filtered, img_dir, tokenizer, train_ds, tf)\nval_dataset = ChestXRayDataset(reports_filtered, projections_filtered, img_dir, tokenizer, val_ds, tf)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"BATCH_SIZE = 16\n\n# create the DataLoader to generate batches of the dataset and iterate over them\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)\nval_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, num_workers=4, pin_memory=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### **Importing the EncoderCNN**","metadata":{}},{"cell_type":"code","source":"def conv_layer(n_input, n_output, kernel_size, stride=1):\n    return nn.Sequential(\n        nn.Conv2d(n_input, n_output, kernel_size, stride),\n        nn.ReLU(),\n        nn.BatchNorm2d(n_output),\n        nn.MaxPool2d(2)\n    )","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"encoder = nn.Sequential(\n            conv_layer(1, 64, 3),\n            conv_layer(64, 128, 3),\n            conv_layer(128, 256, 3),\n            conv_layer(256, 512, 3)\n        )\n\nencoder.load_state_dict(torch.load(\"/kaggle/input/encodercnn/pytorch/default/1/encoder.pth\"))\nencoder.to(device)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### **Visualize the latent space generated**","metadata":{}},{"cell_type":"code","source":"data_iter = iter(val_loader)\ninputs, _, _ = next(data_iter)\n\ninputs = inputs.to(device)\n\nwith torch.no_grad():\n    latent_space = encoder(inputs)\n\ninputs = inputs.cpu().numpy()\nlatent_space = latent_space.cpu().numpy()\n\nfor idx in range(2):\n    reconstructed = latent_space[idx, 0]\n\n    print(f\"{idx+1}) Latent Space (dim={len(latent_space[idx, 0])}) -> {reconstructed}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **Build and train the FF mapper model**","metadata":{}},{"cell_type":"code","source":"def linear_layer(dim_input, dim_output, drop_p=0.1, last=False):\n    layers = [nn.Linear(dim_input, dim_output)]\n    if not last:\n        layers.append(nn.ReLU())\n        layers.append(nn.Dropout(p=drop_p))\n    return nn.Sequential(*layers)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class FF_mapper(nn.Module):\n\n    def __init__(self, dim_input, dim_output):\n        super().__init__()\n        self.ff = nn.Sequential(\n            linear_layer(dim_input, 640),\n            linear_layer(640, 896),\n            #linear_layer(896, 1024),\n            linear_layer(896, dim_output, last=True),\n            nn.LayerNorm(dim_output)\n        )\n        \n\n    def forward(self, latent_space):\n        # flatter, permute and stuff\n        batch_size, C, H, W = latent_space.shape\n        latent_space = latent_space.permute(0, 2, 3, 1)  # (1, 12, 12, 512)\n        latent_space = latent_space.view(batch_size, H * W, C)  # (1, 144, 512)\n        return self.ff(latent_space)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### The ***soft_generate*** function uses the forward pass of the transformer model to generate the logits. Then, it applies the ***Softmax*** function to obtain a proper probability distribution that will then be used to get the most probable embedding","metadata":{}},{"cell_type":"code","source":"def soft_generate(inputs_embeds, attention_mask, labels, temperature=1.0):\n    outputs = transformerModel(\n        inputs_embeds=inputs_embeds, \n        attention_mask=attention_mask,\n        labels=labels,\n        return_dict=True\n    )\n    logits = outputs.logits  # [batch, seq_len, vocab_size]\n    # Apply softmax with temperature to get differentiable probabilities\n    soft_tokens = nn.functional.softmax(logits / temperature, dim=-1)\n    return soft_tokens  # This is a differentiable approximation","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train(train_x, val_x, model, epochs=10):\n    criterion = mse_cos_sim_loss\n    alpha = 0.0    # used in mixed loss\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n\n    history = []\n    log_template = \"\\nEpoch {ep:03d} train_loss: {t_loss:0.4f} val_loss: {val_loss:0.4f}\"\n\n    with tqdm(desc=\"epoch\", total=epochs) as pbar_outer:\n        for epoch in range(epochs):\n            \n            train_loss = fit_epoch(model, train_x, criterion, optimizer, alpha)\n            val_loss = eval_epoch(model, val_x, criterion, alpha)\n            print(\"loss: \", train_loss)\n\n            history.append((train_loss,val_loss))\n\n            pbar_outer.update(1)\n            tqdm.write(log_template.format(ep=epoch+1, t_loss=train_loss, val_loss=val_loss))            \n        \n    return history\n\ndef cosine_similarity_loss(predicted_embedding, target_embedding):\n    return 1 - nn.functional.cosine_similarity(predicted_embedding, target_embedding, dim=-1).mean()\n\ndef mse_cos_sim_loss(pred, true, alpha=0.05, rescale=10):\n    # try with a rescale factor of 10\n    return alpha * nn.functional.mse_loss(pred, true) + (1-alpha) * rescale * cosine_similarity_loss(pred, true)\n\ndef fit_epoch(model, train_x, criterion, optimizer, alpha):\n    running_loss = 0.0\n    processed_data = 0\n\n    # for epoch progress\n    old_progress = -0.1\n    new_progress = 0\n\n    for idx, (images, text, attention) in enumerate(train_x):\n\n        new_progress = idx/len(train_x)\n        if (new_progress-old_progress >= 0.1):\n            print(f\"Epoch progress: {new_progress*100}%\")\n            old_progress = new_progress\n\n        images = images.to(device)\n        text = text.to(device)\n        attention = attention.to(device)\n\n        optimizer.zero_grad()\n\n        # Get latent space representation\n        with torch.no_grad():\n            latent_space = encoder(images).to(device)\n\n        # FFNN generates transformer input embeddings\n        pred_embeds = model(latent_space)\n\n        # generating the probability distribution\n        pred_prob = soft_generate(pred_embeds, attention, text)\n\n        emb_layer = transformerModel.get_input_embeddings()\n        vocab_embeddings = emb_layer.weight\n        true_y = emb_layer(text)\n        pred_y = torch.matmul(pred_prob, vocab_embeddings)\n\n        # Compute loss\n        loss = criterion(pred_y, true_y, alpha)\n\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item() * images.shape[0]\n        processed_data += images.shape[0]\n        \n    return running_loss / processed_data\n\ndef eval_epoch(model, val_x, criterion, alpha):\n    running_loss = 0.0\n    processed_data = 0\n    model.eval()\n\n    with torch.no_grad():\n        for images, text, attention in val_x:\n            \n            images = images.to(device)\n            text = text.to(device)\n            attention = attention.to(device)\n\n            # using imported models to create the data we need\n            latent_space = encoder(images).to(device)\n\n            pred_embeds = model(latent_space)\n        \n            # generating the probability distribution\n            pred_prob = soft_generate(pred_embeds, attention, text)\n\n            emb_layer = transformerModel.get_input_embeddings()\n            vocab_embeddings = emb_layer.weight\n            true_y = emb_layer(text)\n            pred_y = torch.matmul(pred_prob, vocab_embeddings)\n            \n            # Compute loss\n            loss = criterion(pred_y, true_y, alpha)\n            \n            running_loss += loss.item() * images.shape[0]\n            processed_data += images.shape[0]\n    \n    return running_loss / processed_data","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"mapper = FF_mapper(512, hidden_size).to(device)    # hidden_size = 768 for GPT2 and 1024 for BioGPT\n\nmapper_parameters= sum(p.numel() for p in mapper.parameters())\nprint(f\"number of mapper parameters: {mapper_parameters}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import time\n\nstart = time.time()\nhistory = train(train_loader, val_loader, mapper, epochs=20)\nprint(f\"Training duration: {(time.time() - start) / 60} (min)\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_loss, val_loss = zip(*history)\nplt.figure(figsize=(15,10))\nplt.plot(train_loss, label='Train loss')\nplt.plot(val_loss, label='Val loss')\nplt.legend(loc='best')\nplt.xlabel(\"epochs\")\nplt.ylabel(\"loss\")\nplt.plot();","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def generate_text(inputs_embeds, attention_mask):\n    \"\"\"can be used only in inference, not in train\"\"\"\n    return transformerModel.generate(\n        inputs_embeds=inputs_embeds, \n        max_length=288,\n        attention_mask=attention_mask,\n        pad_token_id=tokenizer.eos_token_id,\n        no_repeat_ngram_size=2,   # avoid repetitions\n        #top_k=50,   # considers only the 50 most probable words\n        eos_token_id=None,\n        do_sample=False\n    )","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data_iter = iter(train_loader)\nimage, text, attention = next(data_iter)\n\nprint(f\"Real Text:\\n{tokenizer.decode(text[0], skip_special_tokens=True)}\\n\\n\")\n\nimage = image.to(device)\ntext = text.to(device)\nattention = attention.to(device)\n\nwith torch.no_grad():\n    latent_space = encoder(image).to(device)\n    predicted_embedding = mapper(latent_space).to(device)    \n\npredicted_text = generate_text(predicted_embedding, attention)\n\nprint(f\"Predicted Text:\\n{tokenizer.decode(predicted_text[0], skip_special_tokens=True)}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch.save(mapper.state_dict(), f\"ff_mapper_{model_name}.pth\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}